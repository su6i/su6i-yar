import os
import re
import sys
import asyncio
import logging
import subprocess
import signal
import warnings
# Suppress Pydantic V1 warning on Python 3.14+
warnings.filterwarnings("ignore", category=UserWarning, module="langchain_core._api.deprecation")

from pathlib import Path
from dotenv import load_dotenv
import argparse
import io
import json
import uuid
import urllib.parse
import urllib.request
import edge_tts
import html
import httpx
from bs4 import BeautifulSoup
import time

# Telegram Imports
from telegram import Update, ReplyKeyboardMarkup, KeyboardButton, ReplyKeyboardRemove, InlineKeyboardButton, InlineKeyboardMarkup, constants
from telegram.constants import ParseMode
from telegram.ext import ApplicationBuilder, ContextTypes, CommandHandler, MessageHandler, filters, CallbackQueryHandler

# LangChain Imports
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.callbacks import AsyncCallbackHandler

# ==============================================================================
# CONFIGURATION & SETUP
# ==============================================================================

# 1. Logging Setup with Custom Formatter
class ColoredFormatter(logging.Formatter):
    """Custom formatter with colors and clean output"""
    
    # ANSI color codes
    COLORS = {
        'DEBUG': '\033[36m',    # Cyan
        'INFO': '\033[32m',     # Green
        'WARNING': '\033[33m',  # Yellow
        'ERROR': '\033[31m',    # Red
        'CRITICAL': '\033[35m', # Magenta
        'RESET': '\033[0m'      # Reset
    }
    
    def format(self, record):
        # Add color to level name
        levelname = record.levelname
        if levelname in self.COLORS:
            record.levelname = f"{self.COLORS[levelname]}{levelname}{self.COLORS['RESET']}"
        
        # Shorten format for cleaner output
        log_fmt = "%(levelname)s - %(name)s - %(message)s"
        formatter = logging.Formatter(log_fmt)
        return formatter.format(record)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("SmartBot")
logger.propagate = False  # Prevent logs from double-appearing in console

# Add colored formatter to console handler
console_handler = logging.StreamHandler()
console_handler.setFormatter(ColoredFormatter())
logger.handlers = [console_handler]
logger.setLevel(logging.INFO)

# Suppress verbose logs from httpx and google_genai
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("google_genai").setLevel(logging.WARNING)
logging.getLogger("google.genai").setLevel(logging.WARNING)
logging.getLogger("google_genai._api_client").setLevel(logging.ERROR)


# 2. Argument Parsing for Environment
parser = argparse.ArgumentParser(description="Su6i Yar Bot")
parser.add_argument("--dev", action="store_true", help="Run in development mode")
args = parser.parse_args()
IS_DEV = args.dev

# 2. Environment Variables
load_dotenv()
if args.dev:
    logger.info("üõ†Ô∏è Running in DEVELOPMENT MODE")
    TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN_DEV") or os.getenv("TELEGRAM_BOT_TOKEN")
else:
    TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")

TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

# 3. Global Settings
SETTINGS = {
    "download": True,
    "fact_check": False,
    "min_fc_len": 200,
    "lang": "fa",
    "admin_id": int(TELEGRAM_CHAT_ID) if TELEGRAM_CHAT_ID else 0,
    "public_mode": False,  # If True, anyone can use. If False, only whitelist.
    "default_daily_limit": 10,  # Default daily AI requests for whitelisted users
    "free_trial_limit": 3,  # Free requests for non-whitelisted users
}

# Rate Limiting (per user)
RATE_LIMIT = {}  # user_id -> last_request_time

# Market Data Caching (tgju.org)
MARKET_DATA_CACHE = None
MARKET_DATA_TIMESTAMP = 0
MARKET_CACHE_TTL = 300  # 5 minutes
RATE_LIMIT_SECONDS = 5  # Minimum seconds between AI requests per user

# Access Control: Whitelist
# Format: user_id -> {"daily_limit": int, "requests_today": int, "last_reset": date}
ALLOWED_USERS = {
    # Admin is always allowed with unlimited access
}

# Access Control: Allowed Groups (empty = all groups if public_mode is True)
ALLOWED_GROUPS = set()  # Add group IDs here, e.g., {-1001234567890}

# Daily request tracking
from datetime import date
USER_DAILY_USAGE = {}  # user_id -> {"count": int, "date": str}
USER_LANG = {}         # user_id -> "fa" | "en" | "fr" | "ko"
SEARCH_FILE_ID = None  # Persistent telegram file_id for the status GIF

PERSISTENCE_FILE = "user_data.json"

def save_persistence():
    """Save user languages and daily usage to file."""
    try:
        data = {
            "user_lang": USER_LANG,
            "user_usage": USER_DAILY_USAGE,
            "search_file_id": SEARCH_FILE_ID
        }
        with open(PERSISTENCE_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logger.error(f"Persistence Save Error: {e}")

def load_persistence():
    """Load user languages and daily usage from file."""
    global USER_LANG, USER_DAILY_USAGE
    if os.path.exists(PERSISTENCE_FILE):
        try:
            with open(PERSISTENCE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                # Convert string keys back to int if needed (JSON keys are always strings)
                USER_LANG = {int(k): v for k, v in data.get("user_lang", {}).items()}
                USER_DAILY_USAGE = {int(k): v for k, v in data.get("user_usage", {}).items()}
                global SEARCH_FILE_ID
                SEARCH_FILE_ID = data.get("search_file_id")
                logger.info(f"üìÅ Loaded persistence: {len(USER_LANG)} users, {len(USER_DAILY_USAGE)} usage, GIF: {'Exists' if SEARCH_FILE_ID else 'None'}")
        except Exception as e:
            logger.error(f"Persistence Load Error: {e}")

# Initial load
load_persistence()

def get_user_limit(user_id: int) -> int:
    """Get user's daily request limit."""
    admin_id = SETTINGS["admin_id"]
    if user_id == admin_id:
        return 999  # Unlimited for admin
    
    # Whitelisted users get their custom limit or default
    if user_id in ALLOWED_USERS:
        return ALLOWED_USERS[user_id].get("daily_limit", SETTINGS["default_daily_limit"])
    
    # Non-whitelisted users get free trial limit
    return SETTINGS["free_trial_limit"]

def extract_text(response) -> str:
    """Safely extract text from LangChain response, handling both string and list content."""
    if not response or not hasattr(response, 'content'):
        return ""
    
    content = response.content
    if isinstance(content, list):
        # Handle list-based content (Multimodal/Grounding parts from Gemini)
        return "".join([part.get("text", "") if isinstance(part, dict) else str(part) for part in content]).strip()
    
    return str(content).strip()

def smart_split(text, header="", max_len=1024, overflow_prefix="... ÿßÿØÿßŸÖŸá ÿØÿ± Ÿæ€åÿßŸÖ ÿ®ÿπÿØ€å"):
    """
    Split text into two parts: a caption (max_len) and overflow_text.
    Uses HTML formatting for stability.
    Returns (final_caption_html, overflow_raw_text)
    """
    if not text:
        return header, ""
        
    # Split by paragraphs
    paragraphs = text.split('\n\n')
    current_caption_raw = ""
    overflow_text_raw = ""
    overflow_started = False
    
    for para in paragraphs:
        if overflow_started:
            overflow_text_raw += ("\n\n" if overflow_text_raw else "") + para
        else:
            potential = (current_caption_raw + "\n\n" if current_caption_raw else "") + para
            
            # Test length with HTML escaping
            test_caption_html = header + "\n\n" + html.escape(potential) + "\n\n<i>" + html.escape(overflow_prefix) + "</i>"
            
            if len(test_caption_html) <= max_len:
                current_caption_raw = potential
            else:
                if not current_caption_raw:
                    # Hard split if first paragraph is too long
                    allowed = max_len - len(header) - len(overflow_prefix) - 30
                    current_caption_raw = para[:allowed]
                    overflow_text_raw = para[allowed:]
                    overflow_started = True
                else:
                    overflow_started = True
                    overflow_text_raw = para
                    
    final_caption_html = header + (("\n\n" + html.escape(current_caption_raw)) if current_caption_raw else "")
    if overflow_text_raw:
        final_caption_html += "\n\n<i>" + html.escape(overflow_prefix) + "</i>"
        
    return final_caption_html, overflow_text_raw

async def detect_language(text: str) -> str:
    """Detect language of text. Prioritizes local regex for FA/KO, then AI."""
    if not text:
        return "fa"
        
    # Heuristic for Persian/Arabic
    if re.search(r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]', text):
        return "fa"
    
    # Heuristic for Korean (Hangul)
    if re.search(r'[\uAC00-\uD7AF\u1100-\u11FF]', text):
        return "ko"
        
    # Use AI for EN vs FR or others
    try:
        # Use a very short, fast prompt
        chain = get_smart_chain(grounding=False)
        response = await chain.ainvoke(f"Return only the 2-letter ISO code for this text's language: {text[:100]}")
        content = extract_text(response)
        code = content.lower()[:2]
        return LANG_ALIASES.get(code, code) if code in LANG_ALIASES else code
    except:
        return "en"

def check_access(user_id: int, chat_id: int = None) -> tuple[bool, str]:
    """Check if user has access to use the bot. Returns (allowed, reason)."""
    admin_id = SETTINGS["admin_id"]
    
    # Admin always has unlimited access
    if user_id == admin_id:
        return True, "admin"
    
    # Check if public mode
    if SETTINGS["public_mode"]:
        return True, "public"
    
    # Whitelisted users
    if user_id in ALLOWED_USERS:
        # Check group restriction (if in a group)
        if chat_id and chat_id < 0:  # Negative ID = group
            if ALLOWED_GROUPS and chat_id not in ALLOWED_GROUPS:
                return False, "group_not_allowed"
        return True, "whitelisted"
    
    # Non-whitelisted users get free trial (check if they still have quota)
    has_quota, remaining = check_daily_limit(user_id)
    if has_quota:
        return True, "free_trial"
    
    return False, "trial_expired"

def check_daily_limit(user_id: int) -> tuple[bool, int]:
    """Check if user has remaining daily requests. Returns (allowed, remaining)."""
    # Get user's limit
    user_limit = get_user_limit(user_id)
    
    # Admin has unlimited
    if user_limit >= 999:
        return True, 999
    
    # Get today's usage
    today = str(date.today())
    if user_id not in USER_DAILY_USAGE or USER_DAILY_USAGE[user_id]["date"] != today:
        USER_DAILY_USAGE[user_id] = {"count": 0, "date": today}
    
    current_count = USER_DAILY_USAGE[user_id]["count"]
    remaining = user_limit - current_count
    
    return remaining > 0, remaining

def increment_daily_usage(user_id: int) -> int:
    """Increment user's daily usage count. Returns remaining requests."""
    today = str(date.today())
    if user_id not in USER_DAILY_USAGE or USER_DAILY_USAGE[user_id]["date"] != today:
        USER_DAILY_USAGE[user_id] = {"count": 0, "date": today}
    USER_DAILY_USAGE[user_id]["count"] += 1
    save_persistence()
    
    # Return remaining
    user_limit = get_user_limit(user_id)
    return user_limit - USER_DAILY_USAGE[user_id]["count"]

def get_status_text(user_id: int) -> str:
    """Generate localized status message for a user."""
    dl_s = get_msg("dl_on", user_id) if SETTINGS["download"] else get_msg("dl_off", user_id)
    fc_s = get_msg("fc_on", user_id) if SETTINGS["fact_check"] else get_msg("fc_off", user_id)
    info = get_msg("status_fmt", user_id).format(dl=dl_s, fc=fc_s)
    
    # Add user quota info
    has_quota, remaining = check_daily_limit(user_id)
    limit = get_user_limit(user_id)
    
    # Localized User Type
    if user_id == SETTINGS["admin_id"]:
        user_type = get_msg("user_type_admin", user_id)
    elif user_id in ALLOWED_USERS:
        user_type = get_msg("user_type_member", user_id)
    else:
        user_type = get_msg("user_type_free", user_id)
        
    quota_info = (
        f"\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
        f"üë§ **{get_msg('status_label_user', user_id)}:** `{user_id}`\n"
        f"üè∑Ô∏è **{get_msg('status_label_type', user_id)}:** {user_type}\n"
        f"üìä **{get_msg('status_label_quota', user_id)}:** {remaining}/{limit}"
    )
    return info + quota_info


# ==============================================================================
# CALLBACK HANDLER FOR LIVE STATUS UPDATES
# ==============================================================================

class StatusUpdateCallback(AsyncCallbackHandler):
    """Updates Telegram Status Message when AI model starts generating"""
    def __init__(self, status_msg, get_msg_func):
        self.status_msg = status_msg
        self.get_msg = get_msg_func
        self.last_model = None

    async def on_llm_start(self, serialized, prompts, **kwargs):
        """Called when LLM starts - update status with model name"""
        model_raw = "AI Model"
        
        # Extract model name from serialized data
        if "kwargs" in serialized and "model" in serialized["kwargs"]:
            model_raw = serialized["kwargs"]["model"]
        elif "name" in serialized:
            model_raw = serialized["name"]
        elif "id" in serialized:
            # Sometimes model is in id field as a list
            parts = serialized["id"]
            if isinstance(parts, list) and len(parts) > 0:
                # Last element is usually the model name
                model_raw = parts[-1]
        
        # Use exact model name (e.g., "gemini-2.5-flash")
        self.last_model = model_raw
        
        try:
            user_id = getattr(self.status_msg, 'chat_id', 0)
            text = get_msg("analyzing_model", user_id).format(model=model_raw)
            await self.status_msg.edit_text(text, parse_mode='Markdown')
            logger.info(f"üì° Trying model: {model_raw}")
        except Exception as e:
            logger.debug(f"Status update failed: {e}")
            pass  # Ignore flood wait or edit errors

# User Preferences (In-Memory)
USER_LANG = {}
LEARN_LOCK = asyncio.Lock()  # Prevent concurrent /learn requests to avoid API 429s
LEARN_WAITERS = []           # List of {user_id, status_msg, lang} for live queue updates
# Fallback Tenor Animation (Direct link)
SEARCH_GIF_FALLBACK = "https://media1.tenor.com/m/kI2WQAiG3KAAAAAC/waiting.gif"

# ... (Localization Dictionary MESSAGES is unchanged, skipping for brevity) ...


class FallbackErrorCallback(AsyncCallbackHandler):
    """Log errors when a model fails in the fallback chain"""
    async def on_llm_error(self, error: Exception, **kwargs):
        logger.warning(f"‚ö†Ô∏è Model Failure in Chain: {error}")

async def cmd_check_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("‚úÖ Command /check triggered")
    msg = update.message
    if not msg: return
    user_id = update.effective_user.id
    lang = USER_LANG.get(user_id, "fa")

    # Access Control Check
    allowed, reason = check_access(user_id, msg.chat_id)
    if not allowed:
        await msg.reply_text(get_msg("access_denied", user_id))
        return
    
    # Daily Limit Check
    has_quota, remaining = check_daily_limit(user_id)
    if not has_quota:
        limit = get_user_limit(user_id)
        await reply_and_delete(update, context, get_msg("limit_reached", user_id).format(remaining=0, limit=limit), delay=10)
        return

    # Check if reply or arguments
    target_text = ""
    reply_target_id = msg.message_id
    
    if msg.reply_to_message:
        # Check both text and caption (for media messages)
        target_text = msg.reply_to_message.text or msg.reply_to_message.caption or ""
        reply_target_id = msg.reply_to_message.message_id
        
    if not target_text and context.args:
        target_text = " ".join(context.args)
    
    if not target_text:
        await reply_and_delete(update, context, "‚õî Reply to a message or provide text: `/check <text>`", delay=10)
        return

    status_msg = await msg.reply_text(
        get_msg("analyzing", user_id),
        reply_to_message_id=reply_target_id
    )
    
    # Delete the command message itself if in a group
    if msg.chat_id < 0:
        await safe_delete(msg)

    response = await analyze_text_gemini(target_text, status_msg, lang, user_id=user_id)
    
    # Increment usage and get remaining
    remaining = increment_daily_usage(user_id)
    
    await smart_reply(msg, status_msg, response, user_id, lang)
    
    # Show remaining requests (skip for admin)
    if user_id != SETTINGS["admin_id"]:
        limit = get_user_limit(user_id)
        # Use simple message for quota to avoid cluttering, or just log it
        await reply_and_delete(update, context, f"üìä {remaining}/{limit} {get_msg('limit_remaining_count', user_id)}", delay=15, reply_to_message_id=status_msg.message_id)

# ==============================================================================
# LOGIC: SMART CHAIN FACTORY (LANGCHAIN)
# ==============================================================================

def get_smart_chain(grounding=True):
    """Constructs the self-healing AI model chain (8-Layer Defense)"""
    logger.info(f"‚õìÔ∏è Building Smart AI Chain (Grounding: {grounding})...")
    logger.info(f"üîë Keys found: Gemini={'Yes' if GEMINI_API_KEY else 'No'}, DeepSeek={'Yes' if DEEPSEEK_API_KEY else 'No'}")
    
    defaults = {"google_api_key": GEMINI_API_KEY, "temperature": 0.3}

    # 1. Gemini 3 Flash Preview (Primary - Experimental/Fast)
    primary = ChatGoogleGenerativeAI(
        model="gemini-3-flash-preview", 
        **defaults
    )
    
    # Define Fallbacks in Order (Power > Speed)
    fallback_models = [
        "gemini-2.5-pro",                   # 2. Powerhouse
        "gemini-2.5-flash",                 # 3. Balanced
        "gemini-2.5-flash-test",            # 4. Preview variant
        "gemini-2.5-flash-lite",            # 5. Cost-effective
        "gemini-2.0-flash",                 # 6. Reliable Legacy
        "gemini-2.0-flash-lite",            # 7. Fast Legacy
        "gemini-1.5-flash"                  # 8. Ultimate Safety Net
    ]
    
    # Create Google Runnables
    runnables = [ChatGoogleGenerativeAI(model=m, **defaults) for m in fallback_models]
    
    # 8. DeepSeek (Ultimate Fallback)
    if DEEPSEEK_API_KEY:
        deepseek = ChatOpenAI(
            base_url="https://api.deepseek.com", 
            model="deepseek-chat", 
            api_key=DEEPSEEK_API_KEY,
            temperature=0.3
        )
        runnables.append(deepseek)
        
    return primary.with_fallbacks(runnables)

# Global Cache for Details (Simple Dict: user_id -> detail_text)
# In production, use a TTL cache or database.
LAST_ANALYSIS_CACHE = {}

import time

def check_rate_limit(user_id):
    """Check if user can make AI request. Returns True if allowed."""
    now = time.time()
    last_request = RATE_LIMIT.get(user_id, 0)
    if now - last_request < RATE_LIMIT_SECONDS:
        return False
    RATE_LIMIT[user_id] = now
    return True

async def refresh_learn_queue():
    """Update all waiting users about their position in the queue."""
    for index, waiter in enumerate(LEARN_WAITERS):
        try:
            user_id = waiter["user_id"]
            msg_obj = waiter["status_msg"]
            
            # Get the current slide progress if it's the active one
            prog = waiter.get("progress", "")
            base_text = get_msg("learn_designing", user_id)
            if prog:
                base_text = f"{base_text} ({prog})"
            
            # Position Label:
            # If index is 0, they are the 'active' one (Position 1 in queue)
            # but we only show the label to make it clear why it's not starting yet.
            pos_label = get_msg("learn_queue_pos", user_id).format(pos=index + 1)
            
            await msg_obj.edit_caption(
                caption=f"ü™Ñ {base_text}{pos_label}",
                parse_mode=ParseMode.MARKDOWN
            )
        except Exception:
            pass

async def cmd_learn_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Educational tutor: 3 variations with images, definitions, and sentence audio."""
    msg = update.effective_message
    user_id = update.effective_user.id
    
    # Ensure User Lang is initialized immediately
    if user_id not in USER_LANG:
        USER_LANG[user_id] = "fa"
    user_lang = USER_LANG[user_id]
    
    # Check Daily Limit
    if not check_daily_limit(user_id):
        await msg.reply_text(get_msg("learn_quota_exceeded", user_id))
        return

    # Extract target text and language
    target_text = ""
    target_lang = user_lang # Default to user's app language
    
    if msg.reply_to_message:
        target_text = msg.reply_to_message.text or msg.reply_to_message.caption or ""
        if context.args:
            lang_arg = context.args[0].lower()
            if lang_arg in LANG_ALIASES:
                target_lang = LANG_ALIASES[lang_arg]
    elif context.args:
        # Check if first arg is a language code/alias
        lang_arg = context.args[0].lower()
        if lang_arg in LANG_ALIASES:
            target_lang = LANG_ALIASES[lang_arg]
            target_text = " ".join(context.args[1:])
        else:
            target_text = " ".join(context.args)

    if not target_text:
        await msg.reply_text(get_msg("learn_no_text", user_id))
        return

    # 3. Queue Management & Status Message
    original_msg_id = msg.reply_to_message.message_id if msg.reply_to_message else msg.message_id
    
    global SEARCH_FILE_ID
    try:
        status_msg = await msg.reply_animation(
            animation=SEARCH_FILE_ID or SEARCH_GIF_FALLBACK,
            caption=f"ü™Ñ {get_msg('learn_designing', user_id)}",
            reply_to_message_id=original_msg_id,
            parse_mode=ParseMode.MARKDOWN
        )
        # Capture file_id for next time
        if not SEARCH_FILE_ID and status_msg.animation:
            SEARCH_FILE_ID = status_msg.animation.file_id
            save_persistence()
            logger.info(f"üöÄ Captured and cached Search GIF file_id: {SEARCH_FILE_ID}")
    except Exception as e:
        logger.error(f"GIF status failed: {e}")
        # Clear cache if it failed, maybe the file_id is invalid
        if SEARCH_FILE_ID:
            SEARCH_FILE_ID = None
            save_persistence()
        status_msg = await msg.reply_text(get_msg("learn_designing", user_id), reply_to_message_id=original_msg_id)
    
    # Add to waiters and refresh positions
    waiter_entry = {"user_id": user_id, "status_msg": status_msg, "lang": user_lang}
    LEARN_WAITERS.append(waiter_entry)
    await refresh_learn_queue()

    # 4. Wait for Global Lock
    async with LEARN_LOCK:
        try:
            await refresh_learn_queue()
        except: pass
            
        try:
            # 4. Educational AI Call
            logger.info(f"ü§ñ Step 1: Requesting deep educational content from AI in {target_lang}...")
            lang_name = LANG_NAMES.get(target_lang, target_lang)
            explanation_lang = "Persian" if user_lang == "fa" else ("English" if user_lang == "en" else ("French" if user_lang == "fr" else "Korean"))
            chain = get_smart_chain(grounding=False)
            
            educational_prompt = (
                f"SYSTEM ROLE: You are a linguistic tutor. Your student's interface language is '{explanation_lang}'.\n\n"
                f"CORE TASK: The student wants to learn about the concept: '{target_text}' in '{target_lang}'.\n\n"
                f"STRICT LANGUAGE MAPPING (FAILURE TO COMPLY IS UNACCEPTABLE):\n"
                f"1. 'word': MUST be the translation of '{target_text}' into '{target_lang}'.\n"
                f"2. 'sentence': MUST be a complete example sentence ONLY in '{target_lang}'.\n"
                f"3. 'meaning': MUST be a definition/explanation written ONLY in '{explanation_lang}'.\n"
                f"4. 'translation': MUST be the translation of the 'sentence' (field #2) ONLY into '{explanation_lang}'.\n\n"
                f"IMPORTANT: Even if the input '{target_text}' is in '{explanation_lang}' or any other language, you MUST provide ALL explanations (meaning/translation) in '{explanation_lang}'.\n\n"
                f"GRAMMAR RULES (CRITICAL):\n"
                f"- For ALL nouns in '{target_lang}', you MUST provide the word in EXACTLY THREE formats separated by slashes: Indefinite Singular / Definite Singular / Plural (e.g., 'un livre / le livre / des livres' for French, or 'a book / the book / books' for English).\n"
                f"- This 'Triple Format' MUST be used as the 'word' field in the JSON.\n"
                f"- Include phonetics for the '{target_lang}' word.\n\n"
                f"Return ONLY valid JSON in this structure:\n"
                f"{{\n"
                f"  \"valid\": true/false,\n"
                f"  \"lang\": \"detected language of '{target_text}'\",\n"
                f"  \"lang_code\": \"ISO code\",\n"
                f"  \"dict\": \"source dictionary\",\n"
                f"  \"is_correction\": true/false,\n"
                f"  \"suggestion\": \"corrected '{target_text}' if misspelled\",\n"
                f"  \"slides\": [\n"
                f"    {{\n"
                f"      \"word\": \"[{target_lang} terms]\",\n"
                f"      \"phonetic\": \"...\",\n"
                f"      \"meaning\": \"[Explanations ONLY in {explanation_lang}]\",\n"
                f"      \"sentence\": \"[{target_lang} sentence]\",\n"
                f"      \"translation\": \"[Translation ONLY in {explanation_lang}]\",\n"
                f"      \"prompt\": \"A highly detailed English visual description for an AI image generator...\",\n"
                f"      \"keywords\": \"3-4 simple English keywords representing the scene for image search\"\n"
                f"    }},\n"
                f"    ... (exactly 3 variant objects)\n"
                f"  ]\n"
                f"}}\n"
                f"REPLY ONLY WITH JSON."
            )
            
            response = await chain.ainvoke([HumanMessage(content=educational_prompt)])
            content = extract_text(response)
            
            # Clean JSON
            if "```json" in content: content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content: content = content.split("```")[1].split("```")[0].strip()
                
            try:
                res = json.loads(content)
                if not res.get("valid"):
                    await status_msg.edit_caption(
                        caption=get_msg("learn_word_not_found_no_suggestion", user_id).format(word=target_text),
                        parse_mode=ParseMode.MARKDOWN
                    )
                    return

                det_lang = res.get("lang", "Unknown")
                det_lang_code = res.get("lang_code", target_lang)
                det_dict = res.get("dict", "General")
                
                if res.get("is_correction"):
                    suggestion = res.get("suggestion", target_text)
                    await status_msg.edit_caption(
                        caption=get_msg("learn_word_not_found", user_id).format(word=target_text, suggestion=suggestion, lang=det_lang, dict=det_dict),
                        parse_mode=ParseMode.MARKDOWN
                    )
                else:
                    await status_msg.edit_caption(
                        caption=get_msg("learn_searching_stats", user_id).format(word=target_text, lang=det_lang, dict=det_dict),
                        parse_mode=ParseMode.MARKDOWN
                    )

                # Extract slides
                variations = res.get("slides")
                if not variations or not isinstance(variations, list): raise ValueError("Empty slides")
                variations = variations[:3]

            except Exception:
                # Basic fallback
                translated_text = await translate_text(target_text, target_lang)
                img_prompt = await generate_visual_prompt(target_text)
                variations = [{
                    "word": translated_text,
                    "phonetic": "",
                    "meaning": get_msg("learn_fallback_meaning", user_id),
                    "sentence": "Example sentence goes here.",
                    "translation": get_msg("learn_fallback_translation", user_id),
                    "prompt": img_prompt
                }]

            # 5. Sequential Delivery (Download & Send one-by-one)
            logger.info("üé¨ Starting sequential delivery to avoid timeouts...")
            
            for i, var in enumerate(variations):
                # Update progress for queue visibility
                waiter_entry["progress"] = f"{i+1}/3"
                await refresh_learn_queue()

                # If this is the start of sending real content, remove the temporary status GIF
                if i == 0 and status_msg:
                    await safe_delete(status_msg)
                    status_msg = None # Clear to avoid trying to delete again later

                if i > 0: await asyncio.sleep(3.5)
                    
                word = var.get("word", "")
                phonetic = var.get("phonetic", "")
                meaning = var.get("meaning", "")
                sentence = var.get("sentence", "")
                translation = var.get("translation", "")
                img_prompt = var.get("prompt", target_text)
                keywords = var.get("keywords", target_text)
                
                # --- Per-Slide Image Download (Pollinations -> Unsplash Fallback) ---
                image_bytes = None
                max_retries = 3 # Increased retries
                for attempt in range(max_retries + 1):
                    try:
                        if attempt > 0: await asyncio.sleep(attempt * 1.5)
                        encoded = urllib.parse.quote(img_prompt)
                        seed = int(asyncio.get_event_loop().time()) + i + (attempt * 15)
                        url = f"https://pollinations.ai/p/{encoded}?width=1024&height=1024&seed={seed}&nologo=true"
                        
                        def dl():
                            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
                            # Increased timeout to 90s for reliability
                            with urllib.request.urlopen(req, timeout=90) as r: return r.read()
                        
                        image_bytes = await asyncio.to_thread(dl)
                        if image_bytes and len(image_bytes) > 5000: break # Success
                        
                        # If pollination fails on last attempt, try Unsplash
                        if attempt == max_retries:
                            logger.info(f"üõ°Ô∏è Pollinations failed. Trying Unsplash Fallback for slide {i+1}...")
                            clean_keywords = urllib.parse.quote(keywords.replace(",", " "))
                            unsplash_url = f"https://source.unsplash.com/featured/1024x1024/?{clean_keywords}"
                            
                            def dl_unsplash():
                                req = urllib.request.Request(unsplash_url, headers={'User-Agent': 'Mozilla/5.0'})
                                with urllib.request.urlopen(req, timeout=30) as r: return r.read()
                            
                            image_bytes = await asyncio.to_thread(dl_unsplash)
                            if image_bytes and len(image_bytes) > 5000: break

                    except Exception as e:
                        logger.warning(f"Image {i} attempt {attempt+1} failed: {e}")
                        # Fallback to Unsplash immediately if it's a connection error from Pollinations
                        if "pollinations.ai" in str(e):
                            try:
                                logger.info(f"üõ°Ô∏è Immediate Fallback to Unsplash for slide {i+1}...")
                                clean_keywords = urllib.parse.quote(keywords.replace(",", " "))
                                unsplash_url = f"https://source.unsplash.com/featured/1024x1024/?{clean_keywords}"
                                image_bytes = await asyncio.to_thread(lambda: urllib.request.urlopen(unsplash_url, timeout=30).read())
                                if image_bytes and len(image_bytes) > 5000: break
                            except: pass

                        if attempt == max_retries:
                            logger.error(f"Image {i} permanently failed after {max_retries+1} attempts.")

                try:
                    target_flag = LANG_FLAGS.get(target_lang, "üåê")
                    user_flag = LANG_FLAGS.get(user_lang, "üáÆüá∑")
                    
                    caption = (
                        f"üí° **{word}** {phonetic}\n"
                        f"üìù {meaning}\n\n"
                        f"{get_msg('learn_example_sentence', user_id)}\n"
                        f"{target_flag} `{sentence}`\n"
                        f"{user_flag} {translation}\n\n"
                        f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n{get_msg('learn_slide_footer', user_id).format(index=i+1)}"
                    )

                    current_slide_msg = None
                    if image_bytes:
                        # Basic image validation (check for JPEG/PNG magic numbers)
                        is_valid_image = image_bytes.startswith(b'\xff\xd8') or image_bytes.startswith(b'\x89PNG')
                        
                        if is_valid_image:
                            try:
                                photo_buffer = io.BytesIO(image_bytes)
                                photo_buffer.name = f"learn_{i}.jpg"
                                current_slide_msg = await context.bot.send_photo(
                                    chat_id=msg.chat_id,
                                    photo=photo_buffer,
                                    caption=caption,
                                    parse_mode='Markdown',
                                    reply_to_message_id=original_msg_id,
                                    read_timeout=150
                                )
                            except Exception as photo_e:
                                logger.warning(f"üì∏ Photo send failed, falling back to message: {photo_e}")
                                # Fallback if Telegram rejects the valid-looking photo
                                current_slide_msg = await context.bot.send_message(
                                    chat_id=msg.chat_id,
                                    text=caption,
                                    parse_mode='Markdown',
                                    reply_to_message_id=original_msg_id
                                )
                        else:
                            logger.warning(f"üö´ Downloaded bytes for slide {i+1} are not a valid image. Sending text only.")
                            current_slide_msg = await context.bot.send_message(
                                chat_id=msg.chat_id,
                                text=caption,
                                parse_mode='Markdown',
                                reply_to_message_id=original_msg_id
                            )
                    else:
                        current_slide_msg = await context.bot.send_message(
                            chat_id=msg.chat_id,
                            text=caption,
                            parse_mode='Markdown',
                            reply_to_message_id=original_msg_id
                        )
                    
                    # Audio (linked to the SLIDE)
                    # 1. Target Language (Word + Sentence)
                    target_tts = f"{word}. {sentence}"
                    target_audio_buf = await text_to_speech(target_tts, target_lang)
                    
                    # 2. Interface Language (Translation)
                    trans_audio_buf = await text_to_speech(translation, user_lang)
                    
                    # 3. Merge them (Podcast Style)
                    final_audio_buf = await merge_bilingual_audio(target_audio_buf, trans_audio_buf)
                    
                    if final_audio_buf and current_slide_msg:
                        await context.bot.send_voice(
                            chat_id=msg.chat_id,
                            voice=final_audio_buf,
                            caption=f"üîä {word}",
                            reply_to_message_id=current_slide_msg.message_id, # Link audio to its slide
                            read_timeout=120
                        )

                except Exception as item_e:
                    logger.info(f"‚ùå Error sending item {i+1}: {item_e}")
                    try:
                        await context.bot.send_message(
                            chat_id=msg.chat_id,
                            text=f"‚ùå **{word}**\nError: {item_e}",
                            reply_to_message_id=original_msg_id
                        )
                    except: pass

            if not IS_DEV:
                await safe_delete(status_msg)
            
            # FINISHED: Remove from waiters and refresh positions for others
            if waiter_entry in LEARN_WAITERS:
                LEARN_WAITERS.remove(waiter_entry)
            await refresh_learn_queue()
            
            increment_daily_usage(user_id)
            
        except Exception as e:
            logger.error(f"Learn Loop Error: {e}")
            try:
                await status_msg.edit_text(get_msg("learn_error", user_id))
            except: pass


async def analyze_text_gemini(text, status_msg=None, lang_code="fa", user_id=None):
    """Analyze text using Smart Chain Fallback"""
    # Fix: Allow analysis even if disabled in settings (controlled by caller)
    # if not SETTINGS["fact_check"]: return None


    # Map lang_code to English name for Prompt
    lang_map = {"fa": "Persian (Farsi)", "en": "English", "fr": "French"}
    target_lang = lang_map.get(lang_code, "Persian")

    try:
        logger.info(f"üß† STARTING AI ANALYSIS ({target_lang}) for text: {text[:20]}...")
        # Language-specific labels for comparison table
        if lang_code == "fa":
            overall_status_label = "**Ÿàÿ∂ÿπ€åÿ™ ⁄©ŸÑ€å:**"
            comparison_table_label = "**ÿ¨ÿØŸàŸÑ ŸÖŸÇÿß€åÿ≥Ÿá:**"
            text_claim_label = "‚ñ´Ô∏è **ÿßÿØÿπÿß€å ŸÖÿ™ŸÜ:**"
            research_label = "‚ñ´Ô∏è **ŸÖŸÇÿßŸÑÿßÿ™:**"
            conclusion_label = "‚ñ´Ô∏è **ŸÜÿ™€åÿ¨Ÿá ÿ™ÿ≠ŸÇ€åŸÇÿßÿ™:**"
            status_label = "‚ñ´Ô∏è **Ÿàÿ∂ÿπ€åÿ™:**"
            result_label = "**ŸÜÿ™€åÿ¨Ÿá:**"
            example_conclusion1 = "ÿ™ÿ≠ŸÇ€åŸÇÿßÿ™ ÿß€åŸÜ ŸÖ€åÿ≤ÿßŸÜ ÿÆÿ≥ÿ™⁄Ø€å ÿ±ÿß ÿ™ÿ£€å€åÿØ ŸÖ€å‚Äå⁄©ŸÜÿØ"
            example_conclusion2 = "ÿ™ÿ≠ŸÇ€åŸÇÿßÿ™ ⁄©ÿßŸáÿ¥ ÿ™ŸÖÿ±⁄©ÿ≤ ÿ±ÿß ŸÜÿ¥ÿßŸÜ ŸÖ€å‚ÄåÿØŸáÿØ ÿßŸÖÿß ÿØÿ±ÿµÿØ ÿØŸÇ€åŸÇ ŸÖÿ™ŸÅÿßŸàÿ™ ÿßÿ≥ÿ™"
            example_not_specified = "ÿØÿ± ÿ™ÿ≠ŸÇ€åŸÇÿßÿ™ ŸÖÿ¥ÿÆÿµ ŸÜÿ¥ÿØŸá"
        elif lang_code == "en":
            overall_status_label = "**Overall Status:**"
            comparison_table_label = "**Comparison Table:**"
            text_claim_label = "‚ñ´Ô∏è **Text Claim:**"
            research_label = "‚ñ´Ô∏è **Research Papers:**"
            conclusion_label = "‚ñ´Ô∏è **Research Findings:**"
            status_label = "‚ñ´Ô∏è **Status:**"
            result_label = "**Conclusion:**"
            example_conclusion1 = "Research confirms fatigue increases by this amount"
            example_conclusion2 = "Research shows concentration decreases but exact percentage varies"
            example_not_specified = "Not specified in research"
        else:  # French
            overall_status_label = "**Statut Global:**"
            comparison_table_label = "**Tableau de Comparaison:**"
            text_claim_label = "‚ñ´Ô∏è **Affirmation du Texte:**"
            research_label = "‚ñ´Ô∏è **Articles:**"
            conclusion_label = "‚ñ´Ô∏è **R√©sultats de Recherche:**"
            status_label = "‚ñ´Ô∏è **Statut:**"
            result_label = "**Conclusion:**"
            example_conclusion1 = "La recherche confirme cette augmentation de fatigue"
            example_conclusion2 = "La recherche montre une diminution de concentration mais le pourcentage exact varie"
            example_not_specified = "Non sp√©cifi√© dans la recherche"
        
        prompt_text = (
            f"You are a professional Fact-Check Assistant. Answer STRICTLY in **{target_lang}** language.\n\n"
            f"Analyze the following text and provide your response in {target_lang}.\n\n"
            "CRITICAL FORMATTING RULES:\n"
            "1. Your response MUST be split into TWO parts using: |||SPLIT|||\n"
            "2. Use ‚úÖ emoji ONLY for TRUE/VERIFIED claims\n"
            "3. Use ‚ùå emoji ONLY for FALSE/INCORRECT claims\n"
            "4. Use ‚ö†Ô∏è emoji for PARTIALLY TRUE/MISLEADING claims\n"
            "5. DO NOT use bullet points (‚Ä¢) or asterisks (*) - Telegram doesn't support them well\n"
            "6. Add blank lines between paragraphs for readability\n\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "PART 1: SUMMARY (VERY SHORT - Mobile Display)\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "IMPORTANT: Keep this section VERY SHORT (max 500 words)\n"
            "RULE: If the text contains only ONE simple claim, analyze ONLY that claim. DO NOT invent 'implied' claims unless they are dangerous or misleading.\n"
            f"Format EXACTLY like this:\n\n"
            f"{overall_status_label} [‚úÖ/‚ö†Ô∏è/‚ùå]\n\n"
            f"{comparison_table_label}\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            f"{text_claim_label} 17%\n"
            f"{research_label} 17.1%\n"
            f"{conclusion_label} {example_conclusion1}\n"
            f"{status_label} ‚úÖ\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            f"{text_claim_label} 45%\n"
            f"{research_label} {example_not_specified}\n"
            f"{conclusion_label} {example_conclusion2}\n"
            f"{status_label} ‚ö†Ô∏è\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "(Continue for MAX 3-4 claims - each claim MUST be different!)\n\n"
            f"{result_label}\n"
            "[2-3 sentences ONLY]\n\n"
            "|||SPLIT|||\n\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "PART 2: DETAILED ANALYSIS (Complete)\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "CRITICAL: Add blank line between EVERY paragraph for readability!\n"
            "DO NOT use bullet points (‚Ä¢) or asterisks (*)\n"
            "Use simple numbered lists or plain paragraphs\n\n"
            "For each claim:\n"
            "- Full scientific explanation\n"
            "- Exact references with titles and links\n"
            "- Biological/technical mechanisms\n"
            "- Detailed comparison of ALL claimed vs actual data\n"
            "- Academic sources with DOI/URLs\n\n"
            f"Text to analyze:\n{text}"
        )
        
        chain = get_smart_chain()
        logger.info(f"üöÄ Invoking LangChain with 8-Layer Defense for user {user_id}...")
        
        # Add callback for live model name updates
        config = {}
        if status_msg:
            config["callbacks"] = [StatusUpdateCallback(status_msg, get_msg)]
        
        # Invoke Chain (Async) with callbacks
        try:
            # Add error logging callback (Ensure correct instantiation)
            run_config = config.copy() if config else {}
            run_config["callbacks"] = run_config.get("callbacks", []) + [FallbackErrorCallback()]
            
            response = await chain.ainvoke([HumanMessage(content=prompt_text)], config=run_config)

        except Exception as chain_error:
            logger.error(f"üö® CRITICAL CHAIN FAILURE: Type={type(chain_error).__name__} | Msg={chain_error}")
            # Log the full traceback for deep debugging
            import traceback
            logger.error(traceback.format_exc())
            raise # Re-raise to be caught by the outer block which sends 'price_error'
        
        # Final status update with actual model name
        if status_msg:
            model_raw = response.response_metadata.get('model_name', 'gemini-2.5-flash')
            if "token_usage" in response.response_metadata:
                model_raw = "deepseek-chat"
            
            # Use model_raw directly (exact model name like "gemini-2.5-flash")
            model_name = model_raw
            
            try:
                await status_msg.edit_text(
                    get_msg("analysis_complete", user_id).format(model=model_name),
                    parse_mode='Markdown'
                )
            except Exception:
                pass
        
        logger.info(f"‚úÖ Response from {model_name}")
        return response

    except Exception as e:
        logger.error(f"‚ùå SmartChain Error: {e}", exc_info=True)
        return None

# 4. Localization Dictionary
# 4. Localization Dictionary
MESSAGES = {
    "fa": {
        "welcome": (
            "üëã **ÿ≥ŸÑÿßŸÖ {name}!**\n"
            "ÿ®Ÿá **Su6i Yar**ÿå ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ŸÖŸÜÿØ ÿÆŸàÿ¥ ÿ¢ŸÖÿØ€åÿØ.\n\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üîª ÿßÿ≤ ŸÖŸÜŸà€å Ÿæÿß€å€åŸÜ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ €åÿß ŸÑ€åŸÜ⁄© ÿß€åŸÜÿ≥ÿ™ÿß⁄Øÿ±ÿßŸÖ ÿ¨Ÿáÿ™ ÿØÿßŸÜŸÑŸàÿØ ÿ®ŸÅÿ±ÿ≥ÿ™€åÿØ"
        ),
        "btn_status": "üìä Ÿàÿ∂ÿπ€åÿ™ ÿ±ÿ®ÿßÿ™",
        "btn_help": "üÜò ÿ±ÿßŸáŸÜŸÖÿß",
        "btn_dl": "üì• ŸÖÿØ€åÿ±€åÿ™ ÿØÿßŸÜŸÑŸàÿØ",
        "btn_fc": "üß† ÿ±ÿßÿ≥ÿ™€å‚Äåÿ¢ÿ≤ŸÖÿß€å€å",
        "btn_stop": "üõë ÿÆÿßŸÖŸàÿ¥ ⁄©ÿ±ÿØŸÜ ÿ±ÿ®ÿßÿ™",
        "btn_voice": "üîä ÿµŸàÿ™€å",
        "btn_lang_fa": "üáÆüá∑ ŸÅÿßÿ±ÿ≥€å",
        "btn_lang_en": "üá∫üá∏ English",
        "btn_lang_fr": "üá´üá∑ Fran√ßais",
        "status_fmt": (
            "üìä **Ÿàÿ∂ÿπ€åÿ™ ŸÑÿ≠ÿ∏Ÿá‚Äåÿß€å ÿ≥€åÿ≥ÿ™ŸÖ**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üì• **ÿØÿßŸÜŸÑŸàÿØÿ±:**          {dl}\n"
            "üß† **ÿ±ÿßÿ≥ÿ™€å‚Äåÿ¢ÿ≤ŸÖÿß€å€å:**      {fc}\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üîª ÿ®ÿ±ÿß€å ÿ™ÿ∫€å€åÿ± ÿßÿ≤ ÿØ⁄©ŸÖŸá‚ÄåŸáÿß€å ÿ≤€åÿ± ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ"
        ),
        "help_msg": (
            "üìö **ÿ±ÿßŸáŸÜŸÖÿß€å ⁄©ÿßŸÖŸÑ ŸÇÿßÿ®ŸÑ€åÿ™‚ÄåŸáÿß€å ÿ±ÿ®ÿßÿ™**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
            "üì• **ÿØÿßŸÜŸÑŸàÿØÿ± ÿß€åŸÜÿ≥ÿ™ÿß⁄Øÿ±ÿßŸÖ**\n"
            "ŸÑ€åŸÜ⁄© Ÿæÿ≥ÿ™ €åÿß ÿ±€åŸÑÿ≤ ÿ±ÿß ÿ®ŸÅÿ±ÿ≥ÿ™€åÿØ ÿ™ÿß ÿÆŸàÿØ⁄©ÿßÿ± ÿØÿßŸÜŸÑŸàÿØ ÿ¥ŸàÿØ.\n"
            "‚ñ´Ô∏è ÿß⁄Øÿ± ÿØÿßŸÜŸÑŸàÿØ ÿÆŸàÿØ⁄©ÿßÿ± ÿÆÿßŸÖŸàÿ¥ ÿ®ŸàÿØ:\n"
            "`/dl [ŸÑ€åŸÜ⁄©]`\n\n"
            "üß† **ÿ±ÿßÿ≥ÿ™€å‚Äåÿ¢ÿ≤ŸÖÿß€å€å ŸáŸàÿ¥ŸÖŸÜÿØ** (`/check`)\n"
            "ÿ®ÿ±ÿ±ÿ≥€å ÿØÿ±ÿ≥ÿ™€å ÿßÿØÿπÿß €åÿß ÿ™ÿ≠ŸÑ€åŸÑ ŸÖÿ™ŸÜ:\n"
            "‚ñ´Ô∏è ÿ±€åŸæŸÑÿß€å ÿ®Ÿá Ÿæ€åÿßŸÖ:\n"
            "`/check`\n"
            "‚ñ´Ô∏è €åÿß ŸÖÿ≥ÿ™ŸÇ€åŸÖ:\n"
            "`/check [ŸÖÿ™ŸÜ ÿ¥ŸÖÿß]`\n\n"
            "üéì **ÿ¢ŸÖŸàÿ≤ÿ¥ ÿ≤ÿ®ÿßŸÜ** (`/learn`)\n"
            "€åÿßÿØ⁄Ø€åÿ±€å ⁄©ŸÑŸÖÿßÿ™ ÿ®ÿß ÿ™ÿµŸà€åÿ± Ÿà ÿ™ŸÑŸÅÿ∏:\n"
            "‚ñ´Ô∏è ŸÖÿ≥ÿ™ŸÇ€åŸÖ:\n"
            "`/learn [⁄©ŸÑŸÖŸá €åÿß ÿ¨ŸÖŸÑŸá]`\n"
            "‚ñ´Ô∏è ÿ±€åŸæŸÑÿß€å ÿ±Ÿà€å ⁄©ŸÑŸÖŸá:\n"
            "`/learn`\n\n"
            "üîä **ÿ™ÿ®ÿØ€åŸÑ ŸÖÿ™ŸÜ ÿ®Ÿá ÿµŸàÿ™** (`/voice`)\n"
            "‚ñ´Ô∏è ÿÆŸàÿßŸÜÿØŸÜ ŸÖÿ™ŸÜ Ÿæ€åÿßŸÖ (ÿ±€åŸæŸÑÿß€å):\n"
            "`/voice`\n"
            "‚ñ´Ô∏è ÿÆŸàÿßŸÜÿØŸÜ ŸÖÿ™ŸÜ ÿØŸÑÿÆŸàÿßŸá:\n"
            "`/voice [ŸÖÿ™ŸÜ]`\n"
            "‚ñ´Ô∏è ÿ™ÿ±ÿ¨ŸÖŸá Ÿà ÿÆŸàÿßŸÜÿØŸÜ (ŸÖÿ´ŸÑÿßŸã ÿ®Ÿá ÿßŸÜ⁄ØŸÑ€åÿ≥€å):\n"
            "`/voice en [ŸÖÿ™ŸÜ]`\n"
            "*(ÿ≤ÿ®ÿßŸÜ‚ÄåŸáÿß: fa, en, fr, ko)*\n\n"
            "üìä **Ÿàÿ∂ÿπ€åÿ™ Ÿà ÿ≥ŸáŸÖ€åŸá**\n"
            "ŸÖÿ¥ÿßŸáÿØŸá ÿßÿπÿ™ÿ®ÿßÿ± ÿ®ÿßŸÇ€å‚ÄåŸÖÿßŸÜÿØŸá:\n"
            "`/status`\n\n"
            "üí∞ **ŸÜÿ±ÿÆ ÿßÿ±ÿ≤ Ÿà ÿ∑ŸÑÿß**\n"
            "ŸÇ€åŸÖÿ™ ŸÑÿ≠ÿ∏Ÿá‚Äåÿß€å ÿØŸÑÿßÿ±ÿå €åŸàÿ±Ÿà Ÿà ÿ∑ŸÑÿß:\n"
            "`/price`\n\n"
            "üìÑ **ÿ¨ÿ≤ÿ¶€åÿßÿ™ ÿ™ÿ≠ŸÑ€åŸÑ**\n"
            "ÿß⁄Øÿ± ÿ™Ÿàÿ∂€åÿ≠ÿßÿ™ ÿ®€åÿ¥ÿ™ÿ± ÿÆŸàÿßÿ≥ÿ™€åÿØÿå ÿ±Ÿà€å ŸÜÿ™€åÿ¨Ÿá ÿ™ÿ≠ŸÑ€åŸÑ ÿ±€åŸæŸÑÿß€å ⁄©ŸÜ€åÿØ:\n"
            "`/detail`\n\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        ),
        "help_msg_mono": (
            "üìö **ÿ±ÿßŸáŸÜŸÖÿß€å ŸÜÿ≥ÿÆŸá ŸÖŸàŸÜŸà (ÿ™ÿ≥ÿ™)**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
            "üì• **ÿØÿßŸÜŸÑŸàÿØÿ± ÿß€åŸÜÿ≥ÿ™ÿß⁄Øÿ±ÿßŸÖ**\n"
            "```\n"
            "Link       -> Auto Download\n"
            "/dl [Link] -> Force Download\n"
            "```\n"
            "üß† **ÿ±ÿßÿ≥ÿ™€å‚Äåÿ¢ÿ≤ŸÖÿß€å€å**\n"
            "```\n"
            "/check        -> (Reply)\n"
            "/check [Text] -> Direct\n"
            "```\n"
            "üéì **ÿ¢ŸÖŸàÿ≤ÿ¥ ÿ≤ÿ®ÿßŸÜ**\n"
            "```\n"
            "/learn        -> (Reply)\n"
            "/learn [Word] -> Direct\n"
            "```\n"
            "üîä **ÿ™ÿ®ÿØ€åŸÑ ŸÖÿ™ŸÜ ÿ®Ÿá ÿµŸàÿ™**\n"
            "```\n"
            "/voice        -> (Reply)\n"
            "/voice [Text] -> Direct\n"
            "/voice en ... -> Translate\n"
            "```\n"
            "üí∞ **ŸÇ€åŸÖÿ™‚ÄåŸáÿß**\n"
            "```\n"
            "/price        -> Live Rates\n"
            "```\n"
            "üìÑ **ÿ¨ÿ≤ÿ¶€åÿßÿ™**\n"
            "```\n"
            "/detail       -> (Reply)\n"
            "```\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        ),
        "dl_on": "‚úÖ ŸÅÿπÿßŸÑ",
        "dl_off": "‚ùå ÿ∫€åÿ±ŸÅÿπÿßŸÑ",
        "fc_on": "‚úÖ ŸÅÿπÿßŸÑ",
        "fc_off": "‚ùå ÿ∫€åÿ±ŸÅÿπÿßŸÑ",
        "action_dl": "üì• Ÿàÿ∂ÿπ€åÿ™ ÿØÿßŸÜŸÑŸàÿØ: {state}",
        "action_fc": "üß† Ÿàÿ∂ÿπ€åÿ™ ÿ±ÿßÿ≥ÿ™€å‚Äåÿ¢ÿ≤ŸÖÿß€å€å: {state}",
        "lang_set": "üáÆüá∑ ÿ≤ÿ®ÿßŸÜ ÿ±Ÿà€å **ŸÅÿßÿ±ÿ≥€å** ÿ™ŸÜÿ∏€åŸÖ ÿ¥ÿØ",
        "menu_closed": "‚ùå ŸÖŸÜŸà ÿ®ÿ≥ÿ™Ÿá ÿ¥ÿØ. ÿ®ÿ±ÿß€å ÿ®ÿßÿ≤ ⁄©ÿ±ÿØŸÜ /start ÿ®ÿ≤ŸÜ€åÿØ",
        "only_admin": "‚õî ŸÅŸÇÿ∑ ÿßÿØŸÖ€åŸÜ ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ÿß€åŸÜ ⁄©ÿßÿ± ÿ±ÿß ÿßŸÜÿ¨ÿßŸÖ ÿØŸáÿØ",
        "bot_stop": "üõë ÿ±ÿ®ÿßÿ™ ÿØÿ± ÿ≠ÿßŸÑ ÿÆÿßŸÖŸàÿ¥ ÿ¥ÿØŸÜ...",
        "analyzing": "üß† ÿØÿ± ÿ≠ÿßŸÑ ÿ±ÿßÿ≥ÿ™€å‚Äåÿ¢ÿ≤ŸÖÿß€å€å...",
        "too_short": "‚ö†Ô∏è ŸÖÿ™ŸÜ ÿ®ÿ±ÿß€å ÿ™ÿ≠ŸÑ€åŸÑ ÿÆ€åŸÑ€å ⁄©Ÿàÿ™ÿßŸá ÿßÿ≥ÿ™",
        "downloading": "üì• ÿØÿ± ÿ≠ÿßŸÑ ÿØÿßŸÜŸÑŸàÿØ... ŸÑÿ∑ŸÅÿßŸã ÿµÿ®ÿ± ⁄©ŸÜ€åÿØ",
        "uploading": "üì§ ÿØÿ± ÿ≠ÿßŸÑ ÿ¢ŸæŸÑŸàÿØ ÿ®Ÿá ÿ™ŸÑ⁄Øÿ±ÿßŸÖ...",
        "err_dl": "‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿØÿßŸÜŸÑŸàÿØ. ŸÑ€åŸÜ⁄© ÿ±ÿß ÿ®ÿ±ÿ±ÿ≥€å ⁄©ŸÜ€åÿØ",
        "err_too_large": "üö´ ŸÅÿß€åŸÑ ÿ®ÿ≤ÿ±⁄Øÿ™ÿ± ÿßÿ≤ €µ€∞ ŸÖ⁄Øÿßÿ®ÿß€åÿ™ ÿßÿ≥ÿ™ Ÿà ÿ™ŸÑ⁄Øÿ±ÿßŸÖ ÿßÿ¨ÿßÿ≤Ÿá ÿßÿ±ÿ≥ÿßŸÑ ÿ¢ŸÜ ÿ±ÿß ŸÜŸÖ€å‚ÄåÿØŸáÿØ.",
        "err_api": "‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿßÿ±ÿ™ÿ®ÿßÿ∑ ÿ®ÿß ÿ≥ÿ±Ÿàÿ± ÿ™ÿ≠ŸÑ€åŸÑ. ÿ®ÿπÿØÿßŸã ÿ™ŸÑÿßÿ¥ ⁄©ŸÜ€åÿØ",
        "voice_generating": "üîä ÿØÿ± ÿ≠ÿßŸÑ ÿ≥ÿßÿÆÿ™ ŸÅÿß€åŸÑ ÿµŸàÿ™€å...",
        "voice_translating": "üåê ÿØÿ± ÿ≠ÿßŸÑ ÿ™ÿ±ÿ¨ŸÖŸá ÿ®Ÿá {lang}...",
        "voice_caption": "üîä ŸÜÿ≥ÿÆŸá ÿµŸàÿ™€å",
        "voice_caption_lang": "üîä ŸÜÿ≥ÿÆŸá ÿµŸàÿ™€å ({lang})",
        "voice_error": "‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿ≥ÿßÿÆÿ™ ŸÅÿß€åŸÑ ÿµŸàÿ™€å",
        "voice_no_text": "‚õî ÿ®Ÿá €å⁄© Ÿæ€åÿßŸÖ ÿ±€åŸæŸÑÿß€å ÿ®ÿ≤ŸÜ€åÿØ €åÿß ÿßÿ®ÿ™ÿØÿß €å⁄© ŸÖÿ™ŸÜ ÿ±ÿß ÿ™ÿ≠ŸÑ€åŸÑ ⁄©ŸÜ€åÿØ.",
        "voice_invalid_lang": "‚õî ÿ≤ÿ®ÿßŸÜ ŸÜÿßŸÖÿπÿ™ÿ®ÿ±. ÿ≤ÿ®ÿßŸÜ‚ÄåŸáÿß€å Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å: fa, en, fr, ko",
        "access_denied": "‚õî ÿ¥ŸÖÿß ÿØÿ≥ÿ™ÿ±ÿ≥€å ÿ®Ÿá ÿß€åŸÜ ÿ±ÿ®ÿßÿ™ ŸÜÿØÿßÿ±€åÿØ.",
        "limit_reached": "‚õî ÿ≥ŸÇŸÅ ÿØÿ±ÿÆŸàÿßÿ≥ÿ™ ÿ±Ÿàÿ≤ÿßŸÜŸá ÿ¥ŸÖÿß ÿ™ŸÖÿßŸÖ ÿ¥ÿØ ({remaining} ÿßÿ≤ {limit}).",
        "remaining_requests": "üìä ÿØÿ±ÿÆŸàÿßÿ≥ÿ™‚ÄåŸáÿß€å ÿ®ÿßŸÇ€å‚ÄåŸÖÿßŸÜÿØŸá ÿßŸÖÿ±Ÿàÿ≤: {remaining}",
        "learn_designing": "ü™Ñ ÿØÿ± ÿ≠ÿßŸÑ ÿ∑ÿ±ÿßÿ≠€å...",
        "learn_quota_exceeded": "‚ùå ÿ≥ŸáŸÖ€åŸá ÿ±Ÿàÿ≤ÿßŸÜŸá ÿ¥ŸÖÿß ÿ™ŸÖÿßŸÖ ÿ¥ÿØŸá ÿßÿ≥ÿ™.",
        "learn_no_text": "‚ùå ŸÑÿ∑ŸÅÿßŸã ŸÖÿ™ŸÜ €åÿß ⁄©ŸÑŸÖŸá‚Äåÿß€å ÿ®ÿ±ÿß€å €åÿßÿØ⁄Ø€åÿ±€å ÿ®ŸÅÿ±ÿ≥ÿ™€åÿØ (ŸÖÿ´ÿßŸÑ: /learn apple €åÿß ÿØÿ± Ÿæÿßÿ≥ÿÆ ÿ®Ÿá €å⁄© Ÿæ€åÿßŸÖ).",
        "learn_example_sentence": "üìñ **ÿ¨ŸÖŸÑŸá ŸÜŸÖŸàŸÜŸá:**",
        "learn_slide_footer": "üéì *ÿ¢ŸÖŸàÿ≤ÿ¥ ({index}/3)*",
        "learn_queue_pos": " (ŸÜŸÅÿ± {pos} ÿØÿ± ÿµŸÅ...)",
        "learn_word_not_found": "‚ùå ⁄©ŸÑŸÖŸá **{word}** Ÿæ€åÿØÿß ŸÜÿ¥ÿØ.\nÿ¢€åÿß ŸÖŸÜÿ∏Ÿàÿ±ÿ™ÿßŸÜ **{suggestion}** ÿ®ŸàÿØÿü\n(ŸÖŸÜÿ®ÿπ: {lang} - {dict})",
        "learn_word_not_found_no_suggestion": "‚ùå ⁄©ŸÑŸÖŸá **{word}** ÿØÿ± Ÿá€å⁄Ü ÿØ€å⁄©ÿ¥ŸÜÿ±€å ŸÖÿπÿ™ÿ®ÿ±€å Ÿæ€åÿØÿß ŸÜÿ¥ÿØ. ŸÑÿ∑ŸÅÿßŸã ÿßŸÖŸÑÿß€å ÿ¢ŸÜ ÿ±ÿß ÿ®ÿ±ÿ±ÿ≥€å ⁄©ŸÜ€åÿØ.",
        "learn_error": "‚ùå ÿÆÿ∑ÿß€å€å ÿØÿ± ŸÅÿ±ÿ¢€åŸÜÿØ ÿ¢ŸÖŸàÿ≤ÿ¥ ÿ±ÿÆ ÿØÿßÿØ.",
        "learn_fallback_meaning": "ÿ™ÿ±ÿ¨ŸÖŸá ŸÖÿ≥ÿ™ŸÇ€åŸÖ",
        "learn_fallback_translation": "ÿ™ÿ±ÿ¨ŸÖŸá ÿ¨ŸÖŸÑŸá ŸÜŸÖŸàŸÜŸá",
        "status_label_user": "⁄©ÿßÿ±ÿ®ÿ±",
        "status_label_type": "ŸÜŸàÿπ",
        "status_label_quota": "ÿ≥ŸáŸÖ€åŸá ÿßŸÖÿ±Ÿàÿ≤",
        "user_type_admin": "üëë ÿßÿØŸÖ€åŸÜ",
        "user_type_member": "‚úÖ ÿπÿ∂Ÿà",
        "user_type_free": "üÜì ÿ±ÿß€å⁄ØÿßŸÜ",
        "status_private_sent": "‚úÖ Ÿàÿ∂ÿπ€åÿ™ ÿ¥ŸÖÿß ÿ®Ÿá ÿµŸàÿ±ÿ™ ÿÆÿµŸàÿµ€å ÿßÿ±ÿ≥ÿßŸÑ ÿ¥ÿØ.",
        "status_private_error": "‚õî ÿßÿ®ÿ™ÿØÿß €å⁄© ÿ®ÿßÿ± ÿ®Ÿá @su6i\\_yar\\_bot Ÿæ€åÿßŸÖ ÿÆÿµŸàÿµ€å ÿ®ÿØŸá€åÿØ.",
        "analyzing_model": "üß† ÿØÿ± ÿ≠ÿßŸÑ ÿ±ÿßÿ≥ÿ™€å‚Äåÿ¢ÿ≤ŸÖÿß€å€å ÿ®ÿß {model}...",
        "analysis_complete": "‚úÖ ÿ±ÿßÿ≥ÿ™€å‚Äåÿ¢ÿ≤ŸÖÿß€å€å ÿ™Ÿàÿ≥ÿ∑ {model} ÿ™ŸÖÿßŸÖ ÿ¥ÿØ\n(ÿØÿ± ÿ≠ÿßŸÑ ŸÜŸáÿß€å€å ⁄©ÿ±ÿØŸÜ...)",
        "analysis_header": "üß† **ÿ±ÿßÿ≥ÿ™€å‚Äåÿ¢ÿ≤ŸÖÿß€å€å ÿ™Ÿàÿ≥ÿ∑ {model}**",
        "analysis_footer_note": "\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüí° **ÿ®ÿ±ÿß€å ŸÖÿ¥ÿßŸáÿØŸá ÿ¨ÿ≤ÿ¶€åÿßÿ™:**\nÿ®Ÿá ÿß€åŸÜ Ÿæ€åÿßŸÖ ÿ±€åŸæŸÑÿß€å ÿ®ÿ≤ŸÜ€åÿØ Ÿà `/detail` ÿ®ŸÜŸà€åÿ≥€åÿØ",
        "btn_price": "üí∞ ŸÇ€åŸÖÿ™ ÿßÿ±ÿ≤ Ÿà ÿ∑ŸÑÿß",
        "price_loading": "‚è≥ ÿØÿ± ÿ≠ÿßŸÑ ÿØÿ±€åÿßŸÅÿ™ ŸÇ€åŸÖÿ™‚ÄåŸáÿß€å ŸÑÿ≠ÿ∏Ÿá‚Äåÿß€å ÿßÿ≤ tgju.org...",
        "price_error": "‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿØÿ±€åÿßŸÅÿ™ ŸÇ€åŸÖÿ™‚ÄåŸáÿß ÿßÿ≤ tgju.org. ŸÑÿ∑ŸÅÿßŸã ÿØŸàÿ®ÿßÿ±Ÿá ÿ™ŸÑÿßÿ¥ ⁄©ŸÜ€åÿØ.",
        "price_msg": (
            "üí∞ **ŸÇ€åŸÖÿ™ ŸÑÿ≠ÿ∏Ÿá‚Äåÿß€å ÿ®ÿßÿ≤ÿßÿ± (tgju.org)**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üá∫üá∏ **ÿØŸÑÿßÿ±:** `{usd_tm}` ÿ™ŸàŸÖÿßŸÜ\n"
            "üá™üá∫ **€åŸàÿ±Ÿà:** `{eur_tm}` ÿ™ŸàŸÖÿßŸÜ\n"
            "üü° **ÿ∑ŸÑÿß €±€∏ ÿπ€åÿßÿ±:** `{gold18_tm}` ÿ™ŸàŸÖÿßŸÜ\n"
            "**ÿ≠ÿ®ÿßÿ® ÿ∑ŸÑÿß€å €±€∏:** `{diff_tm}`\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üåê **ÿßŸÜÿ≥ ÿ¨ŸáÿßŸÜ€å:** `{ons}`$\n\n"
            "**ÿ∑ŸÑÿß€å €±€∏ ÿ¨ŸáÿßŸÜ€å:**\n"
            "`{theoretical_tm}` ÿ™ŸàŸÖÿßŸÜ"
        ),
        "dl_usage_error": "‚õî ŸÑÿ∑ŸÅÿßŸã ŸÑ€åŸÜ⁄© ÿß€åŸÜÿ≥ÿ™ÿß⁄Øÿ±ÿßŸÖ ÿ±ÿß ÿ®ŸÅÿ±ÿ≥ÿ™€åÿØ €åÿß ÿ±Ÿà€å ÿ¢ŸÜ ÿ±€åŸæŸÑÿß€å ⁄©ŸÜ€åÿØ."
    },
    "en": {
        "welcome": (
            "üëã **Hello {name}!**\n"
            "Welcome to **Su6i Yar**, your AI assistant.\n\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üîª Use the menu below or send a link"
        ),
        "btn_status": "üìä Status",
        "btn_help": "üÜò Help",
        "btn_dl": "üì• Toggle Download",
        "btn_fc": "üß† Toggle AI",
        "btn_stop": "üõë Stop Bot",
        "btn_voice": "üîä Voice",
        "btn_lang_fa": "üáÆüá∑ ŸÅÿßÿ±ÿ≥€å",
        "btn_lang_en": "üá∫üá∏ English",
        "btn_lang_fr": "üá´üá∑ Fran√ßais",
        "status_fmt": (
            "üìä **Live System Status**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üì• **Downloader:**       {dl}\n"
            "üß† **AI Fact-Check:**    {fc}\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üîª Use buttons below to toggle"
        ),
        "help_msg": (
            "üìö **Complete Bot Guide**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
            "üì• **Instagram Downloader:**\n"
            "   ‚Ä¢ Send Post/Reels link\n"
            "   ‚Ä¢ Auto-download in highest quality\n"
            "   ‚Ä¢ Force download: `/dl [link]`\n\n"
            "üß† **Text Analysis (/check):**\n"
            "   ‚Ä¢ Reply to a message: /check\n"
            "   ‚Ä¢ Or directly: /check your text\n"
            "   ‚Ä¢ AI analysis + Google search\n\n"
            "üîä **Voice Conversion (/voice):**\n"
            "   ‚Ä¢ Reply to message: /voice\n"
            "   ‚Ä¢ Or directly: /voice text\n"
            "   ‚Ä¢ Translate + speak: /voice fa text\n"
            "   ‚Ä¢ Languages: fa, en, fr, ko (kr)\n\n"
            "üìÑ **Analysis Details:**\n"
            "   ‚Ä¢ /detail - Get full analysis\n\n"
            "üí∞ **Currency & Gold (/price):**\n"
            "   ‚Ä¢ Live USD, EUR, Gold 18k rates\n"
            "   ‚Ä¢ Gold parity & market gap analysis\n\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        ),
        "help_msg_mono": (
            "üìö **Complete Bot Guide (Mono)**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
            "üì• **Instagram Downloader**\n"
            "```\n"
            "Link       -> Auto Download\n"
            "/dl [Link] -> Force Download\n"
            "```\n"
            "üß† **Fact-Checking**\n"
            "```\n"
            "/check        -> (Reply)\n"
            "/check [Text] -> Direct\n"
            "```\n"
            "üéì **Language Learning**\n"
            "```\n"
            "/learn        -> (Reply)\n"
            "/learn [Word] -> Direct\n"
            "```\n"
            "üîä **Text to Speech**\n"
            "```\n"
            "/voice        -> (Reply)\n"
            "/voice [Text] -> Direct\n"
            "/voice en ... -> Translate\n"
            "```\n"
            "üí∞ **Prices**\n"
            "```\n"
            "/price        -> Live Rates\n"
            "```\n"
            "üìÑ **Details**\n"
            "```\n"
            "/detail       -> (Reply)\n"
            "```\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        ),
        "dl_on": "‚úÖ Active",
        "dl_off": "‚ùå Inactive",
        "fc_on": "‚úÖ Active",
        "fc_off": "‚ùå Inactive",
        "action_dl": "üì• Download status: {state}",
        "action_fc": "üß† AI status: {state}",
        "lang_set": "üá∫üá∏ Language set to **English**",
        "menu_closed": "‚ùå Menu closed. Type /start to reopen",
        "only_admin": "‚õî Admin only",
        "bot_stop": "üõë Bot is shutting down...",
        "analyzing": "üß† Analyzing...",
        "too_short": "‚ö†Ô∏è Text is too short to analyze",
        "downloading": "üì• Downloading... Please wait",
        "uploading": "üì§ Uploading to Telegram...",
        "err_dl": "‚ùå Download failed. Check the link",
        "err_too_large": "üö´ File is larger than 50MB. Telegram doesn't allow sending it via bot.",
        "err_api": "‚ùå AI API error. Try again later",
        "voice_generating": "üîä Generating audio...",
        "voice_translating": "üåê Translating to {lang}...",
        "voice_caption": "üîä Voice version",
        "voice_caption_lang": "üîä Voice version ({lang})",
        "voice_error": "‚ùå Error generating audio",
        "voice_no_text": "‚õî Reply to a message or analyze text first.",
        "voice_invalid_lang": "‚õî Invalid language. Supported: fa, en, fr, ko",
        "access_denied": "‚õî You don't have access to this bot.",
        "limit_reached": "‚õî Daily limit reached ({remaining} of {limit}).",
        "remaining_requests": "üìä Remaining requests today: {remaining}",
        "learn_designing": "ü™Ñ Designing...",
        "learn_quota_exceeded": "‚ùå Daily limit reached.",
        "learn_no_text": "‚ùå Please provide a word or phrase (e.g., /learn apple).",
        "learn_example_sentence": "üìñ **Example Sentence:**",
        "learn_slide_footer": "üéì *Education ({index}/3)*",
        "learn_queue_pos": " (Position {pos} in queue...)",
        "learn_word_not_found": "‚ùå **{word}** not found.\nDid you mean **{suggestion}**?\n(Source: {lang} - {dict})",
        "learn_word_not_found_no_suggestion": "‚ùå Word '**{word}**' was not found in any reliable dictionary. Please check your spelling.",
        "learn_error": "‚ùå An error occurred during the educational process.",
        "learn_fallback_meaning": "Direct translation",
        "learn_fallback_translation": "Example sentence translation",
        "status_label_user": "User",
        "status_label_type": "Type",
        "status_label_quota": "Daily Quota",
        "user_type_admin": "üëë Admin",
        "user_type_member": "‚úÖ Member",
        "user_type_free": "üÜì Free",
        "status_private_sent": "‚úÖ Your status was sent privately.",
        "status_private_error": "‚õî Please send a private message to @su6i\\_yar\\_bot first.",
        "analyzing_model": "üß† Analyzing claims with {model}...",
        "analysis_complete": "‚úÖ Analysis by {model} completed\n(Finalizing response...)",
        "analysis_header": "üß† **Analysis by {model}**",
        "analysis_footer_note": "\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüí° **For full analysis details:**\nReply to this message with `/detail`",
        "btn_price": "üí∞ Currency & Gold",
        "price_loading": "‚è≥ Fetching live rates from tgju.org...",
        "price_error": "‚ùå Error fetching rates from tgju.org. Please try again.",
        "price_msg": (
            "üí∞ **Live Market Rates (tgju.org)**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üá∫üá∏ **USD:** `{usd}` Rial\n"
            "üá™üá∫ **EUR:** `{eur}` Rial\n"
            "üü° **Gold 18k:** `{gold18}` Rial\n"
            "üåê **Global Ounce:** `{ons}`$\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "‚öñÔ∏è **Gold Parity Analysis:**\n"
            "Calculated Price (Ounce to 18k):\n"
            "`{theoretical}` Rial\n"
            "Market Gap: `{diff}` Rial"
        ),
        "dl_usage_error": "‚õî Please provide an Instagram link or reply to one."
    },
    "fr": {
        "welcome": (
            "üëã **Bonjour {name}!**\n"
            "Bienvenue sur **Su6i Yar**, votre assistant IA.\n\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üîª Utilisez le menu ou envoyez un lien"
        ),
        "btn_status": "üìä √âtat",
        "btn_help": "üÜò Aide",
        "btn_dl": "üì• T√©l√©chargement",
        "btn_fc": "üß† IA",
        "btn_stop": "üõë Arr√™ter",
        "btn_voice": "üîä Voix",
        "btn_lang_fa": "üáÆüá∑ ŸÅÿßÿ±ÿ≥€å",
        "btn_lang_en": "üá∫üá∏ English",
        "btn_lang_fr": "üá´üá∑ Fran√ßais",
        "status_fmt": (
            "üìä **√âtat du Syst√®me**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üì• **T√©l√©chargeur:**     {dl}\n"
            "üß† **IA Fact-Check:**    {fc}\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üîª Utilisez les boutons pour changer"
        ),
        "help_msg": (
            "üìö **Guide Complet du Bot**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
            "üì• **T√©l√©chargeur Instagram:**\n"
            "   ‚Ä¢ Envoyez un lien Post/Reels\n"
            "   ‚Ä¢ T√©l√©chargement auto en HD\n"
            "   ‚Ä¢ T√©l√©chargement forc√©: `/dl [lien]`\n\n"
            "üß† **Analyse Texte (/check):**\n"
            "   ‚Ä¢ R√©pondez √† un message: /check\n"
            "   ‚Ä¢ Ou directement: /check texte\n"
            "   ‚Ä¢ Analyse IA + recherche Google\n\n"
            "üîä **Conversion Audio (/voice):**\n"
            "   ‚Ä¢ R√©pondez au message: /voice\n"
            "   ‚Ä¢ Ou directement: /voice texte\n"
            "   ‚Ä¢ Traduire + parler: /voice fa texte\n"
            "   ‚Ä¢ Langues: fa, en, fr, ko (kr)\n\n"
            "üìÑ **D√©tails Analyse:**\n"
            "   ‚Ä¢ /detail - Analyse compl√®te\n\n"
            "üí∞ **Devises & Or (/price):**\n"
            "   ‚Ä¢ Taux USD, EUR, Or 18k en direct\n"
            "   ‚Ä¢ Analyse de parit√© et √©cart du march√©\n\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        ),
        "help_msg_mono": (
            "üìö **Guide Complet du Bot (Mono)**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
            "üì• **T√©l√©chargeur Instagram**\n"
            "```\n"
            "Lien       -> T√©l√©chargement Auto\n"
            "/dl [Lien] -> T√©l√©chargement Forc√©\n"
            "```\n"
            "üß† **V√©rification**\n"
            "```\n"
            "/check        -> (R√©pondre)\n"
            "/check [Text] -> Direct\n"
            "```\n"
            "üéì **Apprentissage**\n"
            "```\n"
            "/learn        -> (R√©pondre)\n"
            "/learn [Mot]  -> Direct\n"
            "```\n"
            "üîä **Synth√®se Vocale**\n"
            "```\n"
            "/voice        -> (R√©pondre)\n"
            "/voice [Text] -> Direct\n"
            "/voice en ... -> Traduire\n"
            "```\n"
            "üí∞ **Prix**\n"
            "```\n"
            "/price        -> Taux en Direct\n"
            "```\n"
            "üìÑ **D√©tails**\n"
            "```\n"
            "/detail       -> (R√©pondre)\n"
            "```\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        ),
        "dl_on": "‚úÖ Actif",
        "dl_off": "‚ùå Inactif",
        "fc_on": "‚úÖ Actif",
        "fc_off": "‚ùå Inactif",
        "action_dl": "üì• T√©l√©chargement: {state}",
        "action_fc": "üß† IA: {state}",
        "lang_set": "üá´üá∑ Langue d√©finie sur **Fran√ßais**",
        "menu_closed": "‚ùå Menu ferm√©. Tapez /start",
        "only_admin": "‚õî Admin seulement",
        "bot_stop": "üõë Arr√™t du bot...",
        "analyzing": "üß† Analyse...",
        "too_short": "‚ö†Ô∏è Texte trop court pour analyser",
        "downloading": "üì• T√©l√©chargement... Patientez",
        "uploading": "üì§ Envoi vers Telegram...",
        "err_dl": "‚ùå √âchec du t√©l√©chargement. V√©rifiez le lien",
        "err_too_large": "üö´ Le fichier d√©passe 50 Mo. Telegram ne permet pas l'envoi via bot.",
        "err_api": "‚ùå Erreur API IA. R√©essayez plus tard",
        "voice_generating": "üîä G√©n√©ration audio...",
        "voice_translating": "üåê Traduction en {lang}...",
        "voice_caption": "üîä Version audio",
        "voice_caption_lang": "üîä Version audio ({lang})",
        "voice_error": "‚ùå Erreur de g√©n√©ration audio",
        "voice_no_text": "‚õî R√©pondez √† un message ou analysez d'abord.",
        "voice_invalid_lang": "‚õî Langue invalide. Support√©es: fa, en, fr, ko",
        "access_denied": "‚õî Vous n'avez pas acc√®s √† ce bot.",
        "limit_reached": "‚õî Limite quotidienne atteinte ({remaining} sur {limit}).",
        "remaining_requests": "üìä Requ√™tes restantes aujourd'hui: {remaining}",
        "learn_designing": "ü™Ñ Conception...",
        "learn_quota_exceeded": "‚ùå Limite quotidienne atteinte.",
        "learn_no_text": "‚ùå Veuillez fournir un mot ou une phrase (ex: /learn apple).",
        "learn_example_sentence": "üìñ **Exemple de phrase:**",
        "learn_slide_footer": "üéì **√âducation ({index}/3)**",
        "learn_searching_stats": "üîç Recherche de **{word}** en {lang} (Source : {dict})...",
        "learn_word_not_found": "‚ö†Ô∏è Mot '**{word}**' introuvable. Affichage des r√©sultats pour '**{suggestion}**' trouv√© en {lang} ({dict}) √† la place...",
        "learn_word_not_found_no_suggestion": "‚ùå Le mot '**{word}**' n'a √©t√© trouv√© dans aucun dictionnaire fiable. Veuillez v√©rifier l'orthographe.",
        "learn_error": "‚ùå Une erreur est survenue pendant le processus √©ducatif.",
        "learn_fallback_meaning": "Traduction directe",
        "learn_fallback_translation": "Traduction de la phrase d'exemple",
        "status_label_user": "Utilisateur",
        "status_label_type": "Type",
        "status_label_quota": "Quota Journalier",
        "user_type_admin": "üëë Admin",
        "user_type_member": "‚úÖ Membre",
        "user_type_free": "üÜì Gratuit",
        "status_private_sent": "‚úÖ Votre √©tat a √©t√© envoy√© en priv√©.",
        "status_private_error": "‚õî Veuillez d'abord envoyer un message priv√© √† @su6i\\_yar\\_bot.",
        "analyzing_model": "üß† Analyse des affirmations avec {model}...",
        "analysis_complete": "‚úÖ Analyse par {model} termin√©e\n(Finalisation de la r√©ponse...)",
        "analysis_header": "üß† **Analyse par {model}**",
        "analysis_footer_note": "\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüí° **Pour les d√©tails de l'analyse:**\nR√©pondez √† ce message avec `/detail`",
        "btn_price": "üí∞ Devises & Or",
        "price_loading": "‚è≥ R√©cup√©ration des taux en direct de tgju.org...",
        "price_error": "‚ùå Erreur lors de la r√©cup√©ration des taux de tgju.org. Veuillez r√©essayer.",
        "price_msg": (
            "üí∞ **Taux du March√© en Direct (tgju.org)**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üá∫üá∏ **USD:** `{usd}` Rial\n"
            "üá™üá∫ **EUR:** `{eur}` Rial\n"
            "üü° **Or 18k:** `{gold18}` Rial\n"
            "üåê **Once Mondiale:** `{ons}`$\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "‚öñÔ∏è **Analyse de la Parit√© de l'Or:**\n"
            "Prix calcul√© (Once √† 18k):\n"
            "`{theoretical}` Rial\n"
            "√âcart du March√©: `{diff}` Rial"
        ),
        "dl_usage_error": "‚õî Veuillez fournir un lien Instagram ou y r√©pondre."
    },
    "ko": {
        "welcome": (
            "üëã **ÏïàÎÖïÌïòÏÑ∏Ïöî {name}!**\n"
            "**Su6i Yar**, AI ÎπÑÏÑúÏóê Ïò§Ïã† Í≤ÉÏùÑ ÌôòÏòÅÌï©ÎãàÎã§.\n\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üîª ÏïÑÎûò Î©îÎâ¥Î•º ÏÇ¨Ïö©ÌïòÍ±∞ÎÇò ÎßÅÌÅ¨Î•º Î≥¥ÎÇ¥ÏÑ∏Ïöî"
        ),
        "btn_status": "üìä ÏÉÅÌÉú",
        "btn_help": "üÜò ÎèÑÏõÄÎßê",
        "btn_dl": "üì• Îã§Ïö¥Î°úÎìú",
        "btn_fc": "üß† AI",
        "btn_stop": "üõë Ï§ëÏßÄ",
        "btn_voice": "üîä ÏùåÏÑ±",
        "btn_lang_fa": "üáÆüá∑ ŸÅÿßÿ±ÿ≥€å",
        "btn_lang_en": "üá∫üá∏ English",
        "btn_lang_fr": "üá´üá∑ Fran√ßais",
        "btn_lang_ko": "üá∞üá∑ ÌïúÍµ≠Ïñ¥",
        "status_fmt": (
            "üìä **ÏãúÏä§ÌÖú ÏÉÅÌÉú**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üì• **Îã§Ïö¥Î°úÎçî:**     {dl}\n"
            "üß† **AI Ìå©Ìä∏Ï≤¥ÌÅ¨:**  {fc}\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üîª Î≤ÑÌäºÏùÑ ÎàåÎü¨ Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî"
        ),
        "help_msg": (
            "üìö **Î¥á Í∞ÄÏù¥Îìú**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
            "üì• **Ïù∏Ïä§ÌÉÄÍ∑∏Îû® Îã§Ïö¥Î°úÎçî:**\n"
            "   ‚Ä¢ Ìè¨Ïä§Ìä∏/Î¶¥Ïä§ ÎßÅÌÅ¨ Ï†ÑÏÜ°\n"
            "   ‚Ä¢ ÏµúÍ≥† ÌôîÏßà ÏûêÎèô Îã§Ïö¥Î°úÎìú\n"
            "   ‚Ä¢ Í∞ïÏ†ú Îã§Ïö¥Î°úÎìú: `/dl [ÎßÅÌÅ¨]`\n\n"
            "üß† **ÌÖçÏä§Ìä∏ Î∂ÑÏÑù (/check):**\n"
            "   ‚Ä¢ Î©îÏãúÏßÄÏóê ÎãµÏû•: /check\n"
            "   ‚Ä¢ ÎòêÎäî ÏßÅÏ†ë: /check ÌÖçÏä§Ìä∏\n"
            "   ‚Ä¢ AI Î∂ÑÏÑù + Íµ¨Í∏Ä Í≤ÄÏÉâ\n\n"
            "üîä **ÏùåÏÑ± Î≥ÄÌôò (/voice):**\n"
            "   ‚Ä¢ Î©îÏãúÏßÄÏóê ÎãµÏû•: /voice\n"
            "   ‚Ä¢ ÎòêÎäî ÏßÅÏ†ë: /voice ÌÖçÏä§Ìä∏\n"
            "   ‚Ä¢ Î≤àÏó≠ + ÎßêÌïòÍ∏∞: /voice fa ÌÖçÏä§Ìä∏\n"
            "   ‚Ä¢ Ïñ∏Ïñ¥: fa, en, fr, ko (kr)\n\n"
            "üìÑ **Î∂ÑÏÑù ÏÉÅÏÑ∏:**\n"
            "   ‚Ä¢ /detail - Ï†ÑÏ≤¥ Î∂ÑÏÑù\n\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        ),
        "help_msg_mono": (
            "üìö **Î¥á Í∞ÄÏù¥Îìú (Mono)**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
            "üì• **Ïù∏Ïä§ÌÉÄÍ∑∏Îû® Îã§Ïö¥Î°úÎçî**\n"
            "```\n"
            "ÎßÅÌÅ¨       -> ÏûêÎèô Îã§Ïö¥Î°úÎìú\n"
            "/dl [ÎßÅÌÅ¨] -> Í∞ïÏ†ú Îã§Ïö¥Î°úÎìú\n"
            "```\n"
            "üß† **Ìå©Ìä∏Ï≤¥ÌÅ¨**\n"
            "```\n"
            "/check        -> (ÎãµÏû•)\n"
            "/check [ÌÖçÏä§Ìä∏] -> ÏßÅÏ†ë\n"
            "```\n"
            "üéì **Ïñ∏Ïñ¥ ÌïôÏäµ**\n"
            "```\n"
            "/learn        -> (ÎãµÏû•)\n"
            "/learn [Îã®Ïñ¥] -> ÏßÅÏ†ë\n"
            "```\n"
            "üîä **ÌÖçÏä§Ìä∏ ÏùåÏÑ± Î≥ÄÌôò**\n"
            "```\n"
            "/voice        -> (ÎãµÏû•)\n"
            "/voice [ÌÖçÏä§Ìä∏] -> ÏßÅÏ†ë\n"
            "/voice en ... -> Î≤àÏó≠\n"
            "```\n"
            "üí∞ **Í∞ÄÍ≤©**\n"
            "```\n"
            "/price        -> Ïã§ÏãúÍ∞Ñ ÌôòÏú®\n"
            "```\n"
            "üìÑ **ÏÉÅÏÑ∏Ï†ïÎ≥¥**\n"
            "```\n"
            "/detail       -> (ÎãµÏû•)\n"
            "```\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        ),
        "dl_on": "‚úÖ ÌôúÏÑ±Ìôî",
        "dl_off": "‚ùå ÎπÑÌôúÏÑ±Ìôî",
        "fc_on": "‚úÖ ÌôúÏÑ±Ìôî",
        "fc_off": "‚ùå ÎπÑÌôúÏÑ±Ìôî",
        "action_dl": "üì• Îã§Ïö¥Î°úÎìú ÏÉÅÌÉú: {state}",
        "action_fc": "üß† AI ÏÉÅÌÉú: {state}",
        "lang_set": "üá∞üá∑ **ÌïúÍµ≠Ïñ¥**Î°ú ÏÑ§Ï†ïÎêòÏóàÏäµÎãàÎã§",
        "menu_closed": "‚ùå Î©îÎâ¥Í∞Ä Îã´ÌòîÏäµÎãàÎã§. /startÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî",
        "only_admin": "‚õî Í¥ÄÎ¶¨Ïûê Ï†ÑÏö©",
        "bot_stop": "üõë Î¥áÏùÑ Ï§ëÏßÄÌï©ÎãàÎã§...",
        "analyzing": "üß† Î∂ÑÏÑù Ï§ë...",
        "too_short": "‚ö†Ô∏è Î∂ÑÏÑùÌïòÍ∏∞Ïóê ÌÖçÏä§Ìä∏Í∞Ä ÎÑàÎ¨¥ ÏßßÏäµÎãàÎã§",
        "downloading": "üì• Îã§Ïö¥Î°úÎìú Ï§ë... Ïû†ÏãúÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî",
        "uploading": "üì§ ÌÖîÎ†àÍ∑∏Îû®Ïóê ÏóÖÎ°úÎìú Ï§ë...",
        "err_dl": "‚ùå Îã§Ïö¥Î°úÎìú Ïã§Ìå®. ÎßÅÌÅ¨Î•º ÌôïÏù∏ÌïòÏÑ∏Ïöî",
        "err_too_large": "üö´ ÌååÏùºÏù¥ 50MBÎ•º Ï¥àÍ≥ºÌï©ÎãàÎã§. ÌÖîÎ†àÍ∑∏Îû® Î¥áÏùÄ 50MB Ïù¥ÏÉÅÏùò ÌååÏùºÏùÑ Î≥¥ÎÇº Ïàò ÏóÜÏäµÎãàÎã§.",
        "err_api": "‚ùå AI API Ïò§Î•ò. ÎÇòÏ§ëÏóê Îã§Ïãú ÏãúÎèÑÌïòÏÑ∏Ïöî",
        "voice_generating": "üîä Ïò§ÎîîÏò§ ÏÉùÏÑ± Ï§ë...",
        "voice_translating": "üåê {lang}Ïóê Î≤àÏó≠ Ï§ë...",
        "voice_caption": "üîä ÏùåÏÑ± Î≤ÑÏ†Ñ",
        "voice_caption_lang": "üîä ÏùåÏÑ± Î≤ÑÏ†Ñ ({lang})",
        "voice_error": "‚ùå Ïò§ÎîîÏò§ ÏÉùÏÑ± Ïò§Î•ò",
        "voice_no_text": "‚õî Î©îÏãúÏßÄÏóê ÎãµÏû•ÌïòÍ±∞ÎÇò Î®ºÏ†Ä ÌÖçÏä§Ìä∏Î•º Î∂ÑÏÑùÌïòÏÑ∏Ïöî.",
        "voice_invalid_lang": "‚õî ÏßÄÏõêÎêòÎäî Ïñ∏Ïñ¥: fa, en, fr, ko",
        "access_denied": "‚õî Ïù¥ Î¥áÏóê Ï†ëÍ∑º Í∂åÌïúÏù¥ ÏóÜÏäµÎãàÎã§.",
        "limit_reached": "‚õî ÏùºÏùº ÌïúÎèÑÏóê ÎèÑÎã¨ÌñàÏäµÎãàÎã§ ({remaining}/{limit}).",
        "remaining_requests": "üìä Ïò§Îäò ÎÇ®ÏùÄ ÏöîÏ≤≠: {remaining}",
        "learn_designing": "ü™Ñ ÎîîÏûêÏù∏ Ï§ë...",
        "learn_quota_exceeded": "‚ùå ÏùºÏùº ÌïúÎèÑÏóê ÎèÑÎã¨ÌñàÏäµÎãàÎã§.",
        "learn_no_text": "‚ùå Îã®Ïñ¥ÎÇò Î¨∏Ïû•ÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî (Ïòà: /learn apple).",
        "learn_example_sentence": "üìñ **ÏòàÎ¨∏:**",
        "learn_slide_footer": "üéì *ÌïôÏäµ ({index}/3)*",
        "learn_queue_pos": " (ÎåÄÍ∏∞ ÏàúÏÑú {pos}Î≤à...)",
        "learn_word_not_found": "‚ùå **{word}** ÏùÑ(Î•º) Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\nÌòπÏãú **{suggestion}** ÏùÑ(Î•º) Ï∞æÏúºÏãúÎÇòÏöî?\n(Ï∂úÏ≤ò: {lang} - {dict})",
        "learn_word_not_found_no_suggestion": "‚ùå **{word}** Îã®Ïñ¥Î•º Ïã†Î¢∞Ìï† Ïàò ÏûàÎäî ÏÇ¨Ï†ÑÏóêÏÑú Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Ï≤†ÏûêÎ•º ÌôïÏù∏Ìï¥ Ï£ºÏÑ∏Ïöî.",
        "learn_error": "‚ùå ÍµêÏú° Í≥ºÏ†ï Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.",
        "learn_fallback_meaning": "ÏßÅÏó≠",
        "learn_fallback_translation": "ÏòàÎ¨∏ Î≤àÏó≠",
        "status_label_user": "ÏÇ¨Ïö©Ïûê",
        "status_label_type": "Ïú†Ìòï",
        "status_label_quota": "ÏùºÏùº ÏÇ¨Ïö©Îüâ",
        "user_type_admin": "üëë Í¥ÄÎ¶¨Ïûê",
        "user_type_member": "‚úÖ Î©§Î≤Ñ",
        "user_type_free": "üÜì Î¨¥Î£å",
        "status_private_sent": "‚úÖ ÏÉÅÌÉúÍ∞Ä ÎπÑÍ≥µÍ∞úÎ°ú Ï†ÑÏÜ°ÎêòÏóàÏäµÎãàÎã§.",
        "status_private_error": "‚õî Î®ºÏ†Ä @su6i\\_yar\\_botÏúºÎ°ú Í∞úÏù∏ Î©îÏãúÏßÄÎ•º Î≥¥ÎÇ¥Ï£ºÏÑ∏Ïöî.",
        "analyzing_model": "üß† {model}(Ïúº)Î°ú Î∂ÑÏÑù Ï§ë...",
        "analysis_complete": "‚úÖ {model} Î∂ÑÏÑù ÏôÑÎ£å\n(ÏùëÎãµ Ï§ÄÎπÑ Ï§ë...)",
        "analysis_header": "üß† **{model}Ïùò Î∂ÑÏÑù Í≤∞Í≥º**",
        "analysis_footer_note": "\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüí° **Ï†ÑÏ≤¥ Î∂ÑÏÑù ÏÉÅÏÑ∏ Ï†ïÎ≥¥:**\nÏù¥ Î©îÏãúÏßÄÏóê `/detail`Î°ú ÎãµÏû•ÌïòÏÑ∏Ïöî",
        "btn_price": "üí∞ ÌôòÏú® Î∞è Í∏à ÏãúÏÑ∏",
        "price_loading": "‚è≥ tgju.orgÏóêÏÑú Ïã§ÏãúÍ∞Ñ ÏãúÏÑ∏Î•º Í∞ÄÏ†∏Ïò§Îäî Ï§ë...",
        "price_error": "‚ùå tgju.orgÏóêÏÑú ÏãúÏÑ∏Î•º Í∞ÄÏ†∏Ïò§Îäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏÑ∏Ïöî.",
        "price_msg": (
            "üí∞ **Ïã§ÏãúÍ∞Ñ ÏãúÏû• ÏãúÏÑ∏ (tgju.org)**\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "üá∫üá∏ **ÎØ∏Íµ≠ Îã¨Îü¨ (USD):** `{usd}` Î¶¨Ïïå\n"
            "üá™üá∫ **Ïú†Î°ú (EUR):** `{eur}` Î¶¨Ïïå\n"
            "üü° **18k Í∏à:** `{gold18}` Î¶¨Ïïå\n"
            "üåê **Íµ≠Ï†ú Í∏à Ïò®Ïä§:** `{ons}`$\n"
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            "‚öñÔ∏è **Í∏à ÏãúÏÑ∏ Î∂ÑÏÑù:**\n"
            "Í≥ÑÏÇ∞Îêú Í∞ÄÍ≤© (Ïò®Ïä§ Îãπ 18k):\n"
            "`{theoretical}` Î¶¨Ïïå\n"
            "ÏãúÏû• Ï∞®Ïù¥: `{diff}` Î¶¨Ïïå"
        ),
        "dl_usage_error": "‚õî Ïù∏Ïä§ÌÉÄÍ∑∏Îû® ÎßÅÌÅ¨Î•º Î≥¥ÎÇ¥Í±∞ÎÇò ÎãµÏû•ÌïòÏÑ∏Ïöî."
    }
}

def get_msg(key, user_id=None):
    """Retrieve localized message based on User ID or Global Settings"""
    # 1. Determine user's current language
    lang = "fa"
    if user_id:
        if user_id in USER_LANG:
            lang = USER_LANG[user_id]
        else:
            # First interaction via command? Initialize default
            USER_LANG[user_id] = "fa"
            lang = "fa"
    else:
        lang = SETTINGS.get("lang", "fa")
    
    # 2. Validation & Fallback Logic
    if lang not in MESSAGES: 
        lang = "fa"
    
    # Priority: User Lang Key -> English Key -> Farsi Key -> Empty String
    target_dict = MESSAGES.get(lang, MESSAGES["fa"])
    if key in target_dict:
        return target_dict[key]
    
    # Fallback to English if key missing in target lang
    if key in MESSAGES["en"]:
        return MESSAGES["en"][key]
        
    # Fallback to Farsi as ultimate default
    return MESSAGES["fa"].get(key, "")

# ==============================================================================
# HELPERS: CLEANUP & ERROR REPORTING
# ==============================================================================

async def schedule_countdown_delete(context, chat_id: int, message_id: int, user_message_id: int, 
                                   original_text: str, total_seconds: int = 60, parse_mode: str = 'Markdown'):
    """
    Updates message with countdown timer and deletes after time expires.
    Shows countdown every 10 seconds.
    """
    intervals = [50, 40, 30, 20, 10]  # Show countdown at these seconds remaining
    
    elapsed = 0
    for remaining in intervals:
        if remaining < total_seconds:
            # Calculate how long to sleep until this interval
            sleep_time = (total_seconds - remaining) - elapsed
            if sleep_time > 0:
                await asyncio.sleep(sleep_time)
                elapsed += sleep_time
            
            try:
                countdown_text = f"‚è±Ô∏è {remaining}s\n\n{original_text}"
                await context.bot.edit_message_text(
                    chat_id=chat_id,
                    message_id=message_id,
                    text=countdown_text,
                    parse_mode=parse_mode
                )
            except Exception as e:
                pass  # Message might be already deleted or edited
    
    # Final sleep before deletion
    await asyncio.sleep(10)
    
    try:
        await context.bot.delete_message(chat_id=chat_id, message_id=message_id)
    except Exception:
        pass
    
    try:
        await context.bot.delete_message(chat_id=chat_id, message_id=user_message_id)
    except Exception:
        pass

async def safe_delete(message):
    """Safely delete a message without crashing on BadRequest"""
    if not message: return
    try:
        await message.delete()
    except Exception:
        pass

async def reply_with_countdown(update: Update, context, text: str, delay: int = 60, **kwargs):
    """
    Reply to message with countdown timer (only in groups).
    Returns the reply message object.
    """
    msg = update.message
    if not msg:
        return None
    
    reply_msg = await msg.reply_text(text, **kwargs)
    
    # Only countdown in groups
    if msg.chat_id < 0:
        asyncio.create_task(
            schedule_countdown_delete(
                context=context,
                chat_id=msg.chat_id,
                message_id=reply_msg.message_id,
                user_message_id=msg.message_id,
                original_text=text,
                total_seconds=delay,
                parse_mode=kwargs.get('parse_mode', 'Markdown')
            )
        )
    
    return reply_msg

async def reply_and_delete(update: Update, context: ContextTypes.DEFAULT_TYPE, text: str, delay: int = 15, **kwargs):
    """
    Sends a reply and schedules its deletion if sent in a group.
    """
    msg = update.message
    if not msg: return
    
    reply_msg = await msg.reply_text(text, **kwargs)
    
    # Disable auto-delete in DEV mode
    if IS_DEV:
        logger.info(f"üß™ [DEV] Skipping auto-deletion for msg at {reply_msg.message_id}")
        return reply_msg

    # Only auto-delete in groups (negative chat_id)
    if msg.chat_id < 0:
        # Delete Bot's Reply
        context.job_queue.run_once(
            lambda ctx: ctx.bot.delete_message(chat_id=msg.chat_id, message_id=reply_msg.message_id),
            delay
        )
        # Delete User's Command Message
        context.job_queue.run_once(
            lambda ctx: ctx.bot.delete_message(chat_id=msg.chat_id, message_id=msg.message_id),
            delay
        )
    return reply_msg

async def report_error_to_admin(context: ContextTypes.DEFAULT_TYPE, user_id: int, command: str, error_msg: str):
    """
    Silently reports an error to the admin instead of spamming the group.
    """
    admin_id = SETTINGS["admin_id"]
    if not admin_id: return

    try:
        report = (
            f"‚ùå **Error Report**\n"
            f"üë§ User: `{user_id}`\n"
            f"üíª Command: `{command}`\n"
            f"‚ö†Ô∏è Error: `{error_msg}`"
        )
        await context.bot.send_message(chat_id=admin_id, text=report, parse_mode='Markdown')
    except Exception as e:
        logger.error(f"Failed to send error report to admin: {e}")

# ==============================================================================
# LOGIC: MENU & KEYBOARDS
# ==============================================================================

def get_main_keyboard(user_id):
    """Generate a compact 3-row keyboard for all user types"""
    is_admin = user_id == SETTINGS["admin_id"]
    
    # Row 1: Core Features (Status, Help, Price)
    row1 = [
        KeyboardButton(get_msg("btn_status", user_id)),
        KeyboardButton(get_msg("btn_help", user_id)),
        KeyboardButton(get_msg("btn_price", user_id))
    ]
    
    # Row 2: Dynamic row (Voice + Admin)
    row2 = [KeyboardButton(get_msg("btn_voice", user_id))]
    if is_admin:
        # For admin, we mix Voice with the most critical toggle
        row2.append(KeyboardButton(get_msg("btn_dl", user_id)))
        row2.append(KeyboardButton(get_msg("btn_fc", user_id)))
        # Note: 'Stop Bot' is moved to row2 for admin to stay within 3 rows
        row2.append(KeyboardButton(get_msg("btn_stop", user_id)))
    
    # Row 3: Languages (Always at bottom)
    row3 = [
        KeyboardButton("üáÆüá∑ ŸÅÿßÿ±ÿ≥€å"), 
        KeyboardButton("üá∫üá∏ English"), 
        KeyboardButton("üá´üá∑ Fran√ßais"), 
        KeyboardButton("üá∞üá∑ ÌïúÍµ≠Ïñ¥")
    ]
    
    kb = [row1, row2, row3]
    return ReplyKeyboardMarkup(kb, resize_keyboard=True)

async def send_welcome(update: Update):
    """Send welcome message with menu"""
    user = update.effective_user
    text = get_msg("welcome", user.id).format(name=user.first_name)
    await update.message.reply_text(
        text, 
        parse_mode='Markdown',
        reply_markup=get_main_keyboard(user.id)
    )





# ==============================================================================
# LOGIC: MARKET RATES (tgju.org)
# ==============================================================================

async def fetch_market_data():
    """Scrape USD, EUR, Gold 18k, and Ons from tgju.org with caching"""
    global MARKET_DATA_CACHE, MARKET_DATA_TIMESTAMP
    
    now = time.time()
    if MARKET_DATA_CACHE and (now - MARKET_DATA_TIMESTAMP) < MARKET_CACHE_TTL:
        logger.info("üì° Using cached market data")
        return MARKET_DATA_CACHE

    logger.info("üåê Fetching live market data from tgju.org")
    url = "https://www.tgju.org/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    try:
        async with httpx.AsyncClient(timeout=20, follow_redirects=True) as client:
            resp = await client.get(url, headers=headers)
            resp.raise_for_status()
            
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # Scrape data using verified selectors with fallbacks
        def get_val(selectors):
            if isinstance(selectors, str):
                selectors = [selectors]
            
            for selector in selectors:
                el = soup.select_one(selector)
                if el:
                    # Remove commas and non-numeric chars for calculation, but keep raw for display
                    raw = el.get_text(strip=True)
                    # For Euro particularly, sometimes the text has extra labels, clean it
                    if "€åŸàÿ±Ÿà" in raw: raw = raw.replace("€åŸàÿ±Ÿà", "").strip()
                    val = re.sub(r'[^\d.]', '', raw)
                    if val:
                        return raw, float(val)
            return "N/A", 0.0

        usd_raw, usd_val = get_val(["li#l-price_dollar_rl span span", "tr[data-market-nameslug='price_dollar_rl'] td.market-price"])
        eur_raw, eur_val = get_val([
            "li#l-price_eur span span", 
            "tr[data-market-nameslug='price_eur'] td.market-price",
            "tr[data-market-row='price_eur'] td.market-price"
        ])
        gold18_raw, gold18_val = get_val(["li#l-geram18 span span", "tr[data-market-nameslug='geram18'] td.market-price"])
        ons_raw, ons_val = get_val(["li#l-ons span span", "tr[data-market-nameslug='ons'] td.market-price"])

        if usd_val == 0 or ons_val == 0:
            logger.warning("‚ö†Ô∏è Scraper returned zero for critical values. Check selectors.")
            return None

        # Calculate Theoretical Gold (18k)
        # Formula: (Ons * Dollar) / 31.1034768 * 0.750
        theoretical_val = (ons_val * usd_val) / 31.1034768 * 0.750
        diff_val = gold18_val - theoretical_val
        
        # Format helpers
        def fmt_curr(val): return f"{int(val):,}"
        def fmt_tm(val): return f"{int(val/10):,}"
        
        data = {
            "usd": usd_raw,
            "eur": eur_raw,
            "gold18": gold18_raw,
            "ons": ons_raw,
            "theoretical": fmt_curr(theoretical_val),
            "diff": ("+" if diff_val > 0 else "") + fmt_curr(diff_val),
            # Toman versions for Farsi
            "usd_tm": fmt_tm(usd_val),
            "eur_tm": fmt_tm(eur_val),
            "gold18_tm": fmt_tm(gold18_val),
            "theoretical_tm": fmt_tm(theoretical_val),
            "diff_tm": ("+" if diff_val > 0 else "") + fmt_tm(diff_val)
        }
        
        MARKET_DATA_CACHE = data
        MARKET_DATA_TIMESTAMP = now
        return data

    except Exception as e:
        logger.error(f"‚ùå Scraper Exception: {e}")
        return None

async def cmd_price_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Handle /price command and button"""
    msg = update.message
    user_id = update.effective_user.id
    
    status_msg = await reply_and_delete(update, context, get_msg("price_loading", user_id), delay=60)
    
    data = await fetch_market_data()
    if not data:
        await status_msg.edit_text(get_msg("price_error", user_id))
        await report_error_to_admin(context, user_id, "/price", "Scraper Failure")
        return

    price_text = get_msg("price_msg", user_id).format(**data)
    await status_msg.edit_text(price_text, parse_mode='Markdown')
    
    # Auto-delete with countdown in groups
    if msg.chat_id < 0:  # Group chat
        asyncio.create_task(
            schedule_countdown_delete(
                context=context,
                chat_id=msg.chat_id,
                message_id=status_msg.message_id,
                user_message_id=msg.message_id,
                original_text=price_text,
                total_seconds=60,
                parse_mode='Markdown'
            )
        )

# ==============================================================================
# HELPERS
# ==============================================================================

async def smart_reply(msg, status_msg, response, user_id, lang="fa"):
    """Send AI response with formatted model name and /detail instruction"""
    if not response:
        await status_msg.edit_text(get_msg("err_api", user_id))
        return

    # 1. Format Model Name
    model_raw = response.response_metadata.get("model_name", "gemini-2.5-flash")
    if "token_usage" in response.response_metadata:
        model_raw = "deepseek-chat"
    
    model_map = {
        "gemini-2.5-pro": "Gemini 2.5 Pro",
        "gemini-1.5-pro": "Gemini 1.5 Pro",
        "gemini-2.5-flash": "Gemini 2.5 Flash",
        "gemini-2.0-flash": "Gemini 2.0 Flash",
        "gemini-1.5-flash": "Gemini 1.5 Flash",
        "gemini-1.5-flash-8b": "Gemini 1.5 Flash 8B",
        "deepseek-chat": "DeepSeek Chat"
    }
    model_name = model_map.get(model_raw, model_raw.replace("-", " ").title())
    
    # 2. Get Headers and Footers from Dictionary
    header = get_msg("analysis_header", user_id).format(model=model_name)
    footer = get_msg("analysis_footer_note", user_id)
    
    # 3. Parse Split (Summary vs Detail)
    full_content = extract_text(response)
    
    split_marker = "|||SPLIT|||"
    
    if split_marker in full_content:
        parts = full_content.split(split_marker, 1)
        summary_text = parts[0].strip()
        detail_text = parts[1].strip()
        
        # Cache detailed analysis
        LAST_ANALYSIS_CACHE[user_id] = f"{header}\n\n{detail_text}"
        logger.info(f"üíæ Cached {len(detail_text)} chars for user {user_id}")
    else:
        # No split found - send everything as summary
        logger.warning(f"‚ö†Ô∏è No split marker found in response")
        summary_text = full_content
        
        no_detail_msgs = {
            "fa": "‚ö†Ô∏è ÿ¨ÿ≤ÿ¶€åÿßÿ™ ÿ®€åÿ¥ÿ™ÿ±€å ÿØÿ± ÿØÿ≥ÿ™ÿ±ÿ≥ ŸÜ€åÿ≥ÿ™",
            "en": "‚ö†Ô∏è No additional details available",
            "fr": "‚ö†Ô∏è Aucun d√©tail suppl√©mentaire"
        }
        LAST_ANALYSIS_CACHE[user_id] = no_detail_msgs.get(lang, no_detail_msgs["fa"])

    # 4. Construct final message
    final_text = f"{header}\n\n{summary_text}{footer}"
    
    # 5. Send (with chunking if needed)
    max_length = 4000
    if len(final_text) > max_length:
        # Chunk the message
        chunks = [final_text[i:i+max_length] for i in range(0, len(final_text), max_length)]
        for i, chunk in enumerate(chunks):
            try:
                if i == 0:
                    await status_msg.edit_text(chunk, parse_mode='Markdown')
                else:
                    await msg.reply_text(chunk, parse_mode='Markdown')
            except Exception:
                # Fallback without Markdown
                if i == 0:
                    await status_msg.edit_text(chunk, parse_mode=None)
                else:
                    await msg.reply_text(chunk, parse_mode=None)
    else:
        # Normal case
        try:
            logger.info(f"üì§ [User {user_id}] Sending final {len(final_text)} chars response...")
            await status_msg.edit_text(final_text, parse_mode='Markdown')
            logger.info(f"‚úÖ [User {user_id}] Response sent successfully.")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [User {user_id}] Markdown send failed, falling back to plain text: {e}")
            await status_msg.edit_text(final_text, parse_mode=None)

# ==============================================================================
# LOGIC: INSTAGRAM DOWNLOAD (YT-DLP + COBALT FALLBACK)
# ==============================================================================

async def download_instagram_cobalt(url: str, filename: Path) -> bool:
    """Download video using Cobalt API as fallback"""
    logger.info("üõ°Ô∏è Falling back to Cobalt API...")
    try:
        # List of public instances (Official + Community)
        # Strategy: Prioritize known-good community instances
        # Source: https://instances.cobalt.best & https://cobalt.directory
        instances = [
            "https://coapi.kelig.me/api/json",         # v7 style
            "https://cobalt.meowing.de",              # High reliability
            "https://cobalt.pub",                     # Community 1
            "https://api.cobalt.kwiatekmiki.pl",      # Community 2
            "https://cobalt.hyperr.net",              # Community 3
            "https://cobalt.kuba2k2.com"             # Additional Community
        ]
        
        # Enhanced headers to mimic browser
        headers = {
            "Accept": "application/json",
            "Content-Type": "application/json",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Origin": "https://cobalt.tools",
            "Referer": "https://cobalt.tools/"
        }

        async with httpx.AsyncClient(timeout=20, follow_redirects=True) as client:
            # Strategy: Try each instance
            for base_url in instances:
                # Handle endpoint differences
                # v7 uses /api/json, v10 uses /
                # We try both implicitly by constructing full URLs or base
                
                # Clean base URL
                base = base_url.rstrip("/")
                if base.endswith("/api/json"):
                    api_url = base # v7 style
                else:
                    api_url = base # v10 style (often root)

                logger.info(f"üõ°Ô∏è Trying Cobalt Instance: {api_url}")

                # Define Payloads (v10 vs v7)
                payloads_to_try = [
                    # v10 Syntax
                    {
                        "url": url,
                        "videoQuality": "max",
                        "audioFormat": "mp3",
                        "filenameStyle": "basic"
                    },
                    # v7 Syntax (Legacy)
                    {
                        "url": url,
                        "vCodec": "h264",
                        "vQuality": "max",
                        "aFormat": "mp3",
                        "filenamePattern": "basic"
                    }
                ]

                dl_url = None
                for i, payload in enumerate(payloads_to_try):
                    try:
                        logger.info(f"üõ∞Ô∏è [Cobalt] Payload {i+1} trial for {api_url}...")
                        resp = await client.post(api_url, json=payload, headers=headers)
                        if resp.status_code not in [200, 201]:
                             logger.warning(f"  > [Cobalt] Payload {i+1} HTTP {resp.status_code} Failure: {resp.text}")
                             continue
                             
                        data = resp.json()
                        if data.get("status") in ["error", "redirect"]:
                             logger.warning(f"  > [Cobalt] API level error: {data.get('text')}")
                             continue

                        dl_url = data.get("url")
                        if not dl_url and data.get("picker"):
                            dl_url = data["picker"][0]["url"]
                        
                        if dl_url:
                            logger.info(f"üîó [Cobalt] Successfully extracted stream URL: {dl_url[:50]}...")
                            break 
                    except Exception as loop_e:
                        logger.error(f"üí• [Cobalt] Exception during payload {i+1} on {api_url}: {str(loop_e)}")
                        continue 

                if dl_url:
                    # Found a working URL from this instance!
                    logger.info(f"‚úÖ Found working Cobalt instance: {api_url}")
                    
                    # Download File Stream
                    try:
                        logger.info("‚¨áÔ∏è Downloading stream from Cobalt...")
                        async with client.stream("GET", dl_url) as dl_resp:
                            dl_resp.raise_for_status()
                            with open(filename, "wb") as f:
                                async for chunk in dl_resp.aiter_bytes():
                                    f.write(chunk)
                        return True
                    except Exception as dl_e:
                        logger.error(f"Stream Download Failed: {dl_e}")
                        # Try next instance if download fails
                        continue 

        logger.error("‚ùå All Cobalt instances failed.")
        return False
    except Exception as e:
        logger.error(f"Cobalt Fallback Logic Failed: {e}")
        return False
async def convert_to_mac_compatible(input_path: Path) -> bool:
    """Re-encode video to H.264/AAC with yuv420p for Mac compatibility"""
    output_path = input_path.with_name(f"fixed_{input_path.name}")
    logger.info(f"üîÑ Converting {input_path.name} for Mac compatibility...")
    
    cmd = [
        "ffmpeg", "-y",
        "-i", str(input_path),
        "-c:v", "libx264",
        "-pix_fmt", "yuv420p",
        "-preset", "faster",
        "-c:a", "aac",
        "-b:a", "128k",
        "-movflags", "+faststart",
        str(output_path)
    ]
    
    try:
        process = await asyncio.create_subprocess_exec(
            *cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        if process.returncode == 0 and output_path.exists():
            logger.info(f"‚úÖ Conversion successful: {output_path.name}")
            # Replace original with fixed version
            input_path.unlink()
            output_path.rename(input_path)
            return True
        else:
            logger.error(f"‚ùå ffmpeg failed (Code {process.returncode}): {stderr.decode()[:200]}")
            if output_path.exists(): output_path.unlink()
            return False
    except Exception as e:
        logger.error(f"üí• ffmpeg Exception: {e}")
        if output_path.exists(): output_path.unlink()
        return False

async def download_instagram(url, chat_id, bot, reply_to_message_id=None):
    """Download and send video using yt-dlp with multi-stage fallback (Anonymous -> Cookies -> Cobalt)"""
    logger.info(f"üöÄ [Chat {chat_id}] Initialization of Instagram download for: {url}")
    
    # Clean URL (Remove tracking parameters for better compatibility)
    if "?" in url:
        original_url = url
        url = url.split("?")[0]
        logger.info(f"üßπ URL cleaned: '{original_url}' -> '{url}'")
        
    try:
        # 1. Filename setup
        timestamp = int(asyncio.get_event_loop().time())
        filename = Path(f"insta_{timestamp}.mp4")
        info_file = Path(f"insta_{timestamp}.info.json")
        logger.debug(f"üìÇ Temp files initialized: {filename}, {info_file}")
        
        # 2. Command - use absolute path if in venv
        import sys
        venv_bin = Path(sys.executable).parent
        yt_dlp_path = venv_bin / "yt-dlp"
        executable = str(yt_dlp_path) if yt_dlp_path.exists() else "yt-dlp"
        logger.info(f"üõ†Ô∏è Using yt-dlp executable: {executable}")
        
        cmd = [
            executable,
            "-f", "bestvideo[ext=mp4][height<=1080]+bestaudio[ext=m4a]/best[ext=mp4]/best",
            "-o", str(filename),
            "--write-info-json",
            "--no-playlist",
            # Remove --max-filesize to avoid silent skips with exit code 0
            url
        ]
        
        # 3. Cookies if available
        cookie_file = Path("cookies.txt")
        if cookie_file.exists():
            cmd.insert(1, str(cookie_file))
            cmd.insert(1, "--cookies")

        # 4. Run Download (1st Attempt: Anonymous)
        logger.info(f"üì• Attempt 1: Downloading {url} anonymously...")
        process = await asyncio.create_subprocess_exec(
            *cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        # Treatment: Successful download MUST produce a file. 
        # If exit code 0 but no file, consider it a failure.
        if process.returncode != 0 or not filename.exists():
            err_msg = stderr.decode()
            out_msg = stdout.decode()
            logger.warning(f"‚ö†Ô∏è Attempt 1 failed (Code {process.returncode}, File: {filename.exists()}): {err_msg[:300]}")
            if out_msg: logger.debug(f"Attempt 1 stdout: {out_msg[:300]}")

            # 4.5 Attempt 2: With Browser Cookies (Safari)
            logger.info("üì• Attempt 2: Retrying with Safari cookies...")
            cmd_with_cookies = cmd[:-1] + ["--cookies-from-browser", "safari", url]
            process = await asyncio.create_subprocess_exec(
                *cmd_with_cookies, stdout=subprocess.PIPE, stderr=subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0 or not filename.exists():
                logger.warning(f"‚ùå Attempt 2 (Cookies) failed (Code {process.returncode}, File: {filename.exists()})")
                logger.error(f"Full stderr from Attempt 2: {stderr.decode()}")
                logger.warning("üß± Both local yt-dlp attempts failed. Triggering Cobalt API fallback sequence...")
                
                success = await download_instagram_cobalt(url, filename)
                if not success:
                    logger.error(f"üõë [Chat {chat_id}] All download methods exhausted for {url}")
                    return False
                logger.info(f"‚ú® [Chat {chat_id}] Recovery successful via Cobalt!")

        # 6. Check File Size (Final Safety Check)
        if filename.exists():
            filesize = filename.stat().st_size
            filesize_mb = filesize / 1024 / 1024
            logger.info(f"üìä Final file downloaded. Size: {filesize_mb:.2f} MB")
            
            if filesize > 50 * 1024 * 1024:
                logger.error(f"üö´ File size ({filesize_mb:.2f}MB) exceeds Telegram Bot API limit (50MB).")
                
                # Attempt 3: Try compressed resolution (720p or lower)
                if "[height<=1080]" in str(cmd):
                    logger.info("üìâ Attempt 3: Retrying with lower resolution (720p)...")
                    filename.unlink()
                    cmd_720 = [executable, "-f", "bestvideo[ext=mp4][height<=720]+bestaudio[ext=m4a]/best[ext=mp4][height<=720]/best", "-o", str(filename), "--no-playlist", url]
                    process = await asyncio.create_subprocess_exec(*cmd_720, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    await process.communicate()
                    
                    if filename.exists():
                        filesize = filename.stat().st_size
                        filesize_mb = filesize / 1024 / 1024
                        logger.info(f"üìä Compressed (720p) file size: {filesize_mb:.2f} MB")
                        if filesize <= 50 * 1024 * 1024:
                             logger.info("‚úÖ 720p is within limits. Proceeding to send...")
                        else:
                            logger.info("üìâ Attempt 4: Retrying with 480p...")
                            filename.unlink()
                            cmd_480 = [executable, "-f", "bestvideo[ext=mp4][height<=480]+bestaudio[ext=m4a]/best[ext=mp4][height<=480]/best", "-o", str(filename), "--no-playlist", url]
                            process = await asyncio.create_subprocess_exec(*cmd_480, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                            await process.communicate()
                            
                            if filename.exists():
                                filesize = filename.stat().st_size
                                filesize_mb = filesize / 1024 / 1024
                                logger.info(f"üìä Compressed (480p) file size: {filesize_mb:.2f} MB")
                                if filesize <= 50 * 1024 * 1024:
                                     logger.info("‚úÖ 480p is within limits. Proceeding to send...")
                                else:
                                     logger.error("üö´ Even 480p is too large.")
                                     filename.unlink()
                                     if info_file.exists(): info_file.unlink()
                                     return "TOO_LARGE"
                            else:
                                logger.error("‚ùì 480p download failed to produce a file.")
                                return False
                    else:
                        logger.error("‚ùì 720p download failed to produce a file.")
                        return False
                
                else:
                    # Still too big or already tried 720
                    filename.unlink()
                    if info_file.exists(): info_file.unlink()
                    return "TOO_LARGE"
        else:
            logger.error(f"‚ùì Download appeared successful but file '{filename}' is missing on disk.")
            return False

        # 5. Extract caption from info.json
        original_caption = ""
        if info_file.exists():
            try:
                import json
                with open(info_file, 'r', encoding='utf-8') as f:
                    info = json.load(f)
                original_caption = info.get('description', '') or info.get('title', '') or ''
                info_file.unlink()  # Cleanup
            except Exception as e:
                logger.warning(f"Could not read caption: {e}")

        # 6. Build caption with smart_split
        header = f"üì• <b>Su6i Yar</b> | @su6i_yar_bot"
        caption, overflow_text = smart_split(original_caption, header=header, max_len=1024)
        
        # 6.5 Ensure Mac compatibility before sending
        if filename.exists():
            await convert_to_mac_compatible(filename)

        # 7. Send to User
        if filename.exists():
            try:
                with open(filename, "rb") as video_file:
                    video_msg = await bot.send_video(
                        chat_id=chat_id,
                        video=video_file,
                        caption=caption,
                        parse_mode='HTML',
                        reply_to_message_id=reply_to_message_id,
                        supports_streaming=True,
                        read_timeout=150,
                        write_timeout=150
                    )
                
                # Send overflow text as reply to video (multiple parts if needed)
                if overflow_text:
                    # For messages, max is 4096. No header needed for follow-up.
                    # We can use smart_split again or just chunk it.
                    remaining = overflow_text
                    while remaining:
                        chunk = remaining[:4000]
                        remaining = remaining[4000:]
                        await bot.send_message(
                            chat_id=chat_id,
                            text=f"üìù <b>ÿßÿØÿßŸÖŸá ⁄©Ÿæÿ¥ŸÜ:</b>\n\n{html.escape(chunk)}",
                            parse_mode='HTML',
                            reply_to_message_id=video_msg.message_id
                        )
                
                # Cleanup
                filename.unlink()
                return True
            except Exception as send_e:
                logger.error(f"Error sending video/overflow: {send_e}")
                # Try fallback without video or without caption
                return False
        return False
        
    except Exception as e:
        logger.error(f"DL Exception: {e}")
        return False

# ==============================================================================
# HANDLERS
# ==============================================================================

# ==============================================================================
# PROCESSED HANDLERS (DEBUGGING ADDED)
# ==============================================================================

async def cmd_start_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info(f"üöÄ Command /start triggered by {update.effective_user.id}")
    # Use reply_with_countdown for welcome message in group
    user = update.effective_user
    text = get_msg("welcome", user.id).format(name=user.first_name)
    await reply_with_countdown(update, context, text, delay=60, 
                           parse_mode='Markdown', 
                           reply_markup=get_main_keyboard(user.id))

async def cmd_close_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("‚ùå Command /close triggered")
    user_id = update.effective_user.id
    await reply_and_delete(update, context, get_msg("menu_closed", user_id), delay=5, reply_markup=ReplyKeyboardRemove())

async def cmd_status_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("üìä Command /status triggered")
    msg = update.message
    user_id = update.effective_user.id
    
    dl_s = get_msg("dl_on", user_id) if SETTINGS["download"] else get_msg("dl_off", user_id)
    fc_s = get_msg("fc_on", user_id) if SETTINGS["fact_check"] else get_msg("fc_off", user_id)
    info = get_msg("status_fmt", user_id).format(dl=dl_s, fc=fc_s)
    
    # Add user quota info
    full_status = get_status_text(user_id)
    await reply_with_countdown(update, context, full_status, delay=30, parse_mode='Markdown')

async def cmd_toggle_dl_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("üì• Command /toggle_dl triggered")
    SETTINGS["download"] = not SETTINGS["download"]
    state = get_msg("dl_on") if SETTINGS["download"] else get_msg("dl_off")
    await reply_and_delete(update, context, get_msg("action_dl").format(state=state), delay=10)

async def cmd_toggle_fc_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("üß† Command /toggle_fc triggered")
    SETTINGS["fact_check"] = not SETTINGS["fact_check"]
    state = get_msg("fc_on") if SETTINGS["fact_check"] else get_msg("fc_off")
    await reply_and_delete(update, context, get_msg("action_fc").format(state=state), delay=10)

async def cmd_download_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Force download manual override"""
    logger.info("üì• Command /dl triggered")
    msg = update.message
    user_id = update.effective_user.id
    
    # 1. Determine Target (Link & Reply ID)
    target_link = ""
    reply_to_id = msg.message_id  # Default: reply to command
    
    if context.args:
        target_link = context.args[0]
    elif msg.reply_to_message:
        target_link = msg.reply_to_message.text or msg.reply_to_message.caption or ""
        reply_to_id = msg.reply_to_message.message_id  # Fix: Reply to original link
    
    # 2. Extract URL
    match = re.search(r'(https?://(?:www\.)?instagram\.com/\S+)', target_link)
    if match:
        target_link = match.group(1)
    
    # 3. Validate
    if "instagram.com" not in target_link:
        await reply_and_delete(update, context, get_msg("dl_usage_error", user_id))
        return

    # 4. Status Message (Auto-Delete on completion/fail)
    status_msg = await msg.reply_text(
        get_msg("downloading", user_id),
        reply_to_message_id=reply_to_id 
    )
    
    # 5. Execute Download
    try:
        success = await download_instagram(target_link, msg.chat_id, context.bot, reply_to_message_id=reply_to_id)
        if success == "TOO_LARGE":
            if not IS_DEV: await safe_delete(status_msg)
            await reply_and_delete(update, context, get_msg("err_too_large", user_id), delay=15)
        elif success:
            # Video sent successfully, cleanup status
            if not IS_DEV: await safe_delete(status_msg)
            # Cleanup command msg if in group
            if msg.chat_id < 0:
                async def del_cmd(ctx):
                    try: await ctx.bot.delete_message(chat_id=msg.chat_id, message_id=msg.message_id)
                    except: pass
                context.job_queue.run_once(del_cmd, 1)
        else:
            if not IS_DEV: await safe_delete(status_msg)
            # Silent error to admin, generic fade-out to user
            await report_error_to_admin(context, user_id, "/dl", f"Download failed for {target_link}")
            await reply_and_delete(update, context, get_msg("err_dl", user_id), delay=10)
            
    except Exception as e:
        if not IS_DEV: await safe_delete(status_msg)
        await report_error_to_admin(context, user_id, "/dl", str(e))
        await reply_and_delete(update, context, get_msg("err_dl", user_id), delay=10)


async def cmd_stop_bot_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    if user_id != SETTINGS["admin_id"]:
        await update.message.reply_text(get_msg("only_admin"))
        return
    await update.message.reply_text(get_msg("bot_stop"), reply_markup=ReplyKeyboardRemove())
    logger.info("üõë KILLING PROCESS WITH SIGKILL (9)")
    os.kill(os.getpid(), signal.SIGKILL)

async def global_message_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """MASTER HANDLER: Processes ALL text messages"""
    msg = update.message
    if not msg or not msg.text: return
    text = msg.text.strip()
    user = update.effective_user
    user_id = user.id
    
    # Ensure User Lang
    if user_id not in USER_LANG:
        USER_LANG[user_id] = "fa"
    lang = USER_LANG[user_id]

    logger.info(f"üì® Message received: '{text}' from {user.id} ({lang})")

    # --- 1. MENU COMMANDS (Check by Emoji/Start) --- 
    
    # Status
    if text.startswith("üìä"):
        full_status = get_status_text(user_id)
        
        # In groups, send privately
        
        # In groups, send privately
        if msg.chat_id < 0:  # Negative ID = group
            try:
                await context.bot.send_message(
                    chat_id=user_id,
                    text=full_status,
                    parse_mode='Markdown'
                )
                notify = await reply_and_delete(update, context, get_msg("status_private_sent", user_id), delay=10)
            except Exception:
                # User hasn't started private chat with bot
                await reply_and_delete(update, context, get_msg("status_private_error", user_id), delay=15)
        else:
            await reply_and_delete(update, context, full_status, delay=30, parse_mode='Markdown')
        return

    # Language Switching
    if "ŸÅÿßÿ±ÿ≥€å" in text:
        USER_LANG[user_id] = "fa"
        save_persistence()
        await reply_and_delete(update, context, "‚úÖ ÿ≤ÿ®ÿßŸÜ ŸÅÿßÿ±ÿ≥€å ÿßŸÜÿ™ÿÆÿßÿ® ÿ¥ÿØ.", reply_markup=get_main_keyboard(user_id))
        return
    if "English" in text:
        USER_LANG[user_id] = "en"
        save_persistence()
        await msg.reply_text("‚úÖ English language selected.", reply_markup=get_main_keyboard(user_id))
        logger.info(f"üá∫üá∏ User {user_id} switched to English")
        return
    if "Fran√ßais" in text:
        USER_LANG[user_id] = "fr"
        save_persistence()
        await reply_and_delete(update, context, "‚úÖ Langue fran√ßaise s√©lectionn√©e.", reply_markup=get_main_keyboard(user_id))
        return
    if "ÌïúÍµ≠Ïñ¥" in text:
        USER_LANG[user_id] = "ko"
        save_persistence()
        await msg.reply_text("‚úÖ ÌïúÍµ≠Ïñ¥Í∞Ä ÏÑ†ÌÉùÎêòÏóàÏäµÎãàÎã§.", reply_markup=get_main_keyboard(user_id))
        return
    
    # Voice Button
    if text.startswith("üîä"):
        detail_text = LAST_ANALYSIS_CACHE.get(user_id)
        if not detail_text:
            await msg.reply_text("‚õî Ÿá€å⁄Ü ÿ™ÿ≠ŸÑ€åŸÑ ÿ∞ÿÆ€åÿ±Ÿá‚Äåÿ¥ÿØŸá‚Äåÿß€å ŸÖŸàÿ¨ŸàÿØ ŸÜ€åÿ≥ÿ™.")
            return
        status_msg = await msg.reply_text(get_msg("voice_generating", user_id))
        try:
            audio_buffer = await text_to_speech(detail_text, lang)
            await msg.reply_voice(voice=audio_buffer, caption="üîä ŸÜÿ≥ÿÆŸá ÿµŸàÿ™€å ÿ™ÿ≠ŸÑ€åŸÑ")
            await safe_delete(status_msg)
        except Exception as e:
            logger.error(f"TTS Error: {e}")
            await status_msg.edit_text(get_msg("voice_error", user_id))
        return
        
    # Help
    if text.startswith("‚ÑπÔ∏è") or text.startswith("üÜò"):
        # Use monospace help for all languages
        help_mono = get_msg("help_msg_mono", user_id)
        if help_mono:
            await reply_with_countdown(update, context, help_mono, delay=60, parse_mode='Markdown')
        else:
            # Fallback to standard help if mono not available
            help_text = get_msg("help_msg", user_id)
            await reply_with_countdown(update, context, help_text, delay=60, parse_mode='Markdown')
        return

    # Price Check
    if "ŸÇ€åŸÖÿ™ ÿßÿ±ÿ≤ Ÿà ÿ∑ŸÑÿß" in text or "Currency & Gold" in text or "Devises & Or" in text or "ÌôòÏú® Î∞è Í∏à ÏãúÏÑ∏" in text:
        await cmd_price_handler(update, context)
        return

    # Toggle DL
    if text.startswith("üì•"):
        SETTINGS["download"] = not SETTINGS["download"]
        state = get_msg("dl_on", user_id) if SETTINGS["download"] else get_msg("dl_off", user_id)
        await msg.reply_text(get_msg("action_dl", user_id).format(state=state))
        return

    # Toggle FC
    if text.startswith("üß†") or "ÿ±ÿßÿ≥ÿ™€å‚Äåÿ¢ÿ≤ŸÖÿß€å€å" in text:
        SETTINGS["fact_check"] = not SETTINGS["fact_check"]
        state = get_msg("fc_on", user_id) if SETTINGS["fact_check"] else get_msg("fc_off", user_id)
        await msg.reply_text(get_msg("action_fc", user_id).format(state=state))
        return

    # Stop (Button)
    if text.startswith("üõë") and user_id == SETTINGS["admin_id"]:
        logger.info("üõë Stop Button Triggered")
        await msg.reply_text(get_msg("bot_stop", user_id), reply_markup=ReplyKeyboardRemove())
        await asyncio.sleep(1)
        os.kill(os.getpid(), signal.SIGKILL)
        return

    # --- 2. INSTAGRAM LINK CHECK ---
    if "instagram.com" in text:
        if not SETTINGS["download"]:
            await msg.reply_text("‚ö†Ô∏è " + get_msg("dl_off", user_id))
            return
            
        status_msg = await msg.reply_text(
            get_msg("downloading", user_id),
            reply_to_message_id=msg.message_id
        )
        
        success = await download_instagram(text, msg.chat_id, context.bot, msg.message_id)
        if success == "TOO_LARGE":
            await status_msg.edit_text(get_msg("err_too_large", user_id))
            if not IS_DEV: 
                async def del_msg(ctx): await safe_delete(status_msg)
                context.job_queue.run_once(del_msg, 15)
        elif success:
            if not IS_DEV: await safe_delete(status_msg)
        else:
            await status_msg.edit_text(get_msg("err_dl", user_id))
        return

    # --- 3. AI ANALYSIS (Fallback) ---
    
    if SETTINGS["fact_check"] and len(text) >= SETTINGS["min_fc_len"]:
        # Access Control Check
        allowed, reason = check_access(user_id, msg.chat_id)
        if not allowed:
            await msg.reply_text(get_msg("access_denied", user_id))
            return
        
        # Daily Limit Check
        has_quota, remaining = check_daily_limit(user_id)
        if not has_quota:
            limit = get_user_limit(user_id)
            await msg.reply_text(get_msg("limit_reached", user_id).format(remaining=0, limit=limit))
            return
        
        status_msg = await msg.reply_text(
            get_msg("analyzing", user_id),
            reply_to_message_id=msg.message_id
        )
        response = await analyze_text_gemini(text, status_msg, lang, user_id)
        
        # Increment usage and get remaining
        remaining = increment_daily_usage(user_id)
        
        await smart_reply(msg, status_msg, response, user_id, lang)
        
        # Show remaining requests (skip for admin)
        if user_id != SETTINGS["admin_id"]:
            limit = get_user_limit(user_id)
            limit = get_user_limit(user_id)
            await msg.reply_text(
                get_msg("remaining_requests", user_id).format(remaining=remaining, limit=limit),
                reply_to_message_id=status_msg.message_id
            )
        return

# ==============================================================================
# MAIN EXECUTION
# ==============================================================================

async def cmd_detail_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Fetches the cached detailed analysis (Zero-Cost)"""
    logger.info("üîç Command /detail triggered")
    msg = update.message
    user_id = update.effective_user.id
    
    # Check Cache
    detail_text = LAST_ANALYSIS_CACHE.get(user_id)
    
    if not detail_text:
        await msg.reply_text("‚õî Ÿá€å⁄Ü ÿ™ÿ≠ŸÑ€åŸÑ ÿ∞ÿÆ€åÿ±Ÿá‚Äåÿ¥ÿØŸá‚Äåÿß€å ŸÖŸàÿ¨ŸàÿØ ŸÜ€åÿ≥ÿ™. ÿßÿ®ÿ™ÿØÿß €å⁄© ŸÖÿ™ŸÜ ÿ±ÿß ÿ™ÿ≠ŸÑ€åŸÑ ⁄©ŸÜ€åÿØ.")
        return

    # Decide reply target
    reply_target_id = msg.message_id
    if msg.reply_to_message:
        reply_target_id = msg.reply_to_message.message_id

    # Smart chunking: split by paragraphs, not mid-paragraph
    max_length = 3900  # Leave some margin
    
    if len(detail_text) <= max_length:
        # Fits in one message
        try:
            await msg.reply_text(detail_text, parse_mode='Markdown', reply_to_message_id=reply_target_id)
        except Exception:
            await msg.reply_text(detail_text, parse_mode=None, reply_to_message_id=reply_target_id)
    else:
        # ... (rest of chunking logic)
        # Need to chunk - split by paragraphs
        paragraphs = detail_text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            # If adding this paragraph exceeds limit, save current chunk and start new one
            if len(current_chunk) + len(para) + 2 > max_length:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para
            else:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
        
        # Don't forget the last chunk
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # Send all chunks
        for i, chunk in enumerate(chunks):
            try:
                if i == 0:
                    await msg.reply_text(f"{chunk}\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüìÑ ÿ®ÿÆÿ¥ {i+1} ÿßÿ≤ {len(chunks)}", parse_mode='Markdown')
                else:
                    await msg.reply_text(f"üìÑ ÿ®ÿÆÿ¥ {i+1} ÿßÿ≤ {len(chunks)}\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n{chunk}", parse_mode='Markdown')
            except Exception:
                if i == 0:
                    await msg.reply_text(f"{chunk}\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüìÑ ÿ®ÿÆÿ¥ {i+1} ÿßÿ≤ {len(chunks)}", parse_mode=None)
                else:
                    await msg.reply_text(f"üìä ÿ®ÿÆÿ¥ {i+1} ÿßÿ≤ {len(chunks)}\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n{chunk}", parse_mode=None, reply_to_message_id=reply_target_id)
        
    # Delete command in groups
    if msg.chat_id < 0:
        await safe_delete(msg)


# TTS Voice Mapping (Standardized for edge-tts)
TTS_VOICES = {
    "fa": "fa-IR-FaridNeural",   # Persian
    "en": "en-US-GuyNeural",     # English
    "fr": "fr-FR-HenriNeural",   # French
    "ko": "ko-KR-InJoonNeural",  # Korean
    "ar": "ar-SA-HamedNeural",   # Arabic
    "de": "de-DE-ConradNeural",  # German
    "es": "es-ES-AlvaroNeural",  # Spanish
    "it": "it-IT-DiegoNeural",   # Italian
    "ja": "ja-JP-KeitaNeural",   # Japanese
    "zh": "zh-CN-YunxiNeural",   # Chinese
    "ru": "ru-RU-DmitryNeural",  # Russian
    "pt": "pt-PT-RaquelNeural",  # Portuguese
    "hi": "hi-IN-MadhurNeural",  # Hindi
}

async def text_to_speech(text: str, lang: str = "fa") -> io.BytesIO:
    """Convert text to speech using edge-tts. Returns audio as BytesIO."""
    # Ensure lang is standardized key
    lang_key = lang[:2].lower()
    
    # Try exact match, then try finding a voice for the ISO code dynamically
    voice = TTS_VOICES.get(lang_key)
    
    # If not in our internal map, try to construct a fallback or use English
    if not voice:
        # Most edge-tts voices follow [lang]-[COUNTRY]-[Name]Neural
        # But constructing them is brittle, so we prioritize the map and then English.
        voice = TTS_VOICES.get("en")
    
    # Heuristic: If text contains Persian/Arabic chars AND target lang is Persian, 
    # or if no specific voice for requested lang, ensure we use Persian if text looks like it.
    if lang_key == "fa" or lang_key not in TTS_VOICES:
        if re.search(r'[\u0600-\u06FF\uFB50-\uFDFF\uFE70-\uFEFF]', text):
            voice = TTS_VOICES["fa"]
    
    # Clean text for TTS (remove markdown)
    clean_text = re.sub(r'\*\*|‚ñ´Ô∏è|‚îÅ+|‚úÖ|‚ùå|‚ö†Ô∏è|üß†|üìÑ|üí°', '', text)
    clean_text = re.sub(r'\[.*?\]', '', clean_text)  # Remove markdown links
    # Replace slashes with a double pause (two commas + pauses) for natural dictation
    clean_text = clean_text.replace(" / ", ", ... , ... ")
    clean_text = clean_text.strip()
    
    # Limit length for TTS (avoid very long audio)
    if len(clean_text) > 2000:
        clean_text = clean_text[:2000] + "..."
    
    communicate = edge_tts.Communicate(clean_text, voice)
    audio_buffer = io.BytesIO()
    
    async for chunk in communicate.stream():
        if chunk["type"] == "audio":
            audio_buffer.write(chunk["data"])
    
    audio_buffer.seek(0)
    return audio_buffer

async def merge_bilingual_audio(target_audio: io.BytesIO, trans_audio: io.BytesIO) -> io.BytesIO:
    """Merge two audio streams with a silence gap using ffmpeg."""
    import tempfile
    import os
    import subprocess
    
    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            t_path = os.path.join(tmpdir, "target.mp3")
            tr_path = os.path.join(tmpdir, "trans.mp3")
            sil_path = os.path.join(tmpdir, "silence.mp3")
            out_path = os.path.join(tmpdir, "merged.mp3")
            
            with open(t_path, "wb") as f: f.write(target_audio.getvalue())
            with open(tr_path, "wb") as f: f.write(trans_audio.getvalue())
            
            # Generate 1 sec of silence
            subprocess.run([
                "ffmpeg", "-y", "-f", "lavfi", "-i", "anullsrc=r=24000:cl=mono", 
                "-t", "1", "-q:a", "9", sil_path
            ], capture_output=True, check=True)
            
            # Concat: Target -> Silence -> Translation
            cmd = [
                "ffmpeg", "-y",
                "-i", t_path, "-i", sil_path, "-i", tr_path,
                "-filter_complex", "[0:a][1:a][2:a]concat=n=3:v=0:a=1[out]",
                "-map", "[out]", "-acodec", "libmp3lame", "-b:a", "64k", out_path
            ]
            subprocess.run(cmd, capture_output=True, check=True)
            
            if os.path.exists(out_path):
                with open(out_path, "rb") as f:
                    return io.BytesIO(f.read())
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è merge_bilingual_audio failed (likely missing ffmpeg): {e}. Falling back to single-language audio.")
        
    return target_audio # Fallback to just the target language audio

# Language code mapping for /voice command
LANG_ALIASES = {
    "fa": "fa", "farsi": "fa", "persian": "fa", "ŸÅÿßÿ±ÿ≥€å": "fa",
    "en": "en", "english": "en", "ÿßŸÜ⁄ØŸÑ€åÿ≥€å": "en",
    "fr": "fr", "french": "fr", "fran√ßais": "fr", "ŸÅÿ±ÿßŸÜÿ≥Ÿà€å": "fr",
    "ko": "ko", "kr": "ko", "korean": "ko", "ÌïúÍµ≠Ïñ¥": "ko", "⁄©ÿ±Ÿá‚Äåÿß€å": "ko",
    "ar": "ar", "arabic": "ar", "ÿπÿ±ÿ®€å": "ar",
    "de": "de", "german": "de", "ÿ¢ŸÑŸÖÿßŸÜ€å": "de",
    "es": "es", "spanish": "es", "ÿßÿ≥ŸæÿßŸÜ€åÿß€å€å": "es",
    "it": "it", "italian": "it", "ÿß€åÿ™ÿßŸÑ€åÿß€å€å": "it",
    "ja": "ja", "japanese": "ja", "⁄òÿßŸæŸÜ€å": "ja",
    "zh": "zh", "chinese": "zh", "⁄Ü€åŸÜ€å": "zh",
    "ru": "ru", "russian": "ru", "ÿ±Ÿàÿ≥€å": "ru",
    "tr": "tr", "turkish": "tr", "ÿ™ÿ±⁄©€å": "tr",
    "pt": "pt", "portuguese": "pt", "Ÿæÿ±ÿ™ÿ∫ÿßŸÑ€å": "pt",
    "hi": "hi", "hindi": "hi", "ŸáŸÜÿØ€å": "hi"
}

LANG_NAMES = {
    "fa": "ŸÅÿßÿ±ÿ≥€å", "en": "ÿßŸÜ⁄ØŸÑ€åÿ≥€å", "fr": "ŸÅÿ±ÿßŸÜÿ≥Ÿà€å", "ko": "⁄©ÿ±Ÿá‚Äåÿß€å",
    "ar": "ÿπÿ±ÿ®€å", "de": "ÿ¢ŸÑŸÖÿßŸÜ€å", "es": "ÿßÿ≥ŸæÿßŸÜ€åÿß€å€å", "it": "ÿß€åÿ™ÿßŸÑ€åÿß€å€å",
    "ja": "⁄òÿßŸæŸÜ€å", "zh": "⁄Ü€åŸÜ€å", "ru": "ÿ±Ÿàÿ≥€å", "tr": "ÿ™ÿ±⁄©€å",
    "pt": "Ÿæÿ±ÿ™ÿ∫ÿßŸÑ€å", "hi": "ŸáŸÜÿØ€å"
}

LANG_FLAGS = {
    "fa": "üáÆüá∑", "en": "üá∫üá∏", "fr": "üá´üá∑", "ko": "üá∞üá∑"
}

async def translate_text(text: str, target_lang: str) -> str:
    """Translate text to target language using Gemini"""
    lang_name = LANG_NAMES.get(target_lang, "English")
    
    try:
        chain = get_smart_chain(grounding=False)
        prompt = f"Translate the following text to {lang_name}. Only output the translation, no explanations:\n\n{text}"
        response = await chain.ainvoke([HumanMessage(content=prompt)])
        return extract_text(response)
    except Exception as e:
        logger.error(f"Translation error: {e}")
        return text  # Return original if translation fails

async def generate_visual_prompt(text: str) -> str:
    """Generate a short English visual prompt for an image representing the text"""
    try:
        chain = get_smart_chain(grounding=False)
        prompt = f"Generate a short, descriptive English visual prompt (single sentence, no style words) representing the core meaning of this text: '{text}'"
        response = await chain.ainvoke([HumanMessage(content=prompt)])
        return extract_text(response).replace('"', '').replace("'", "")
    except Exception as e:
        logger.error(f"Visual prompt generation error: {e}")
        return "abstract conceptual representation"  # Safe default



async def cmd_voice_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Send voice version of replied message or last analysis.
    Usage: /voice [language]
    Examples: /voice, /voice en, /voice english, /voice ŸÅÿßÿ±ÿ≥€å
    """
    logger.info("üîä Command /voice triggered")
    msg = update.message
    user_id = update.effective_user.id
    user_lang = USER_LANG.get(user_id, "fa")
    
    # Check for language argument
    explicit_target = None
    if context.args:
        lang_arg = context.args[0].lower()
        if lang_arg in LANG_ALIASES:
            explicit_target = LANG_ALIASES[lang_arg]
        # If not a lang alias, we assume it's direct text input later
    
    # Priority 1: Check if replied to a message
    target_text = ""
    reply_target_id = msg.message_id
    if msg.reply_to_message:
        target_text = msg.reply_to_message.text or msg.reply_to_message.caption or ""
        reply_target_id = msg.reply_to_message.message_id
    
    # Priority 2: Check for direct text input
    if not target_text and context.args:
        if context.args[0].lower() in LANG_ALIASES:
            if len(context.args) > 1:
                target_text = " ".join(context.args[1:])
        else:
            target_text = " ".join(context.args)
    
    # Priority 3: Check cache
    if not target_text:
        target_text = LAST_ANALYSIS_CACHE.get(user_id, "")
        # If from cache, we might not have a good reply target, use command
        reply_target_id = msg.message_id 
    
    if not target_text:
        await reply_and_delete(update, context, get_msg("voice_no_text", user_id), delay=10)
        return

    # Delete command in groups
    if msg.chat_id < 0:
        await safe_delete(msg)

    # Decide target language and translation need
    if explicit_target:
        # User explicitly asked for a specific language -> Translate if needed
        target_lang = explicit_target
        # We assume the source text is usually in the user's interface language for translation purposes,
        # but the translation logic itself handles any source.
        # Actually, let's detect source to be sure if translation is needed.
        source_lang = await detect_language(target_text)
        need_translation = target_lang != source_lang
    else:
        # No language specified -> Use the text's natural language (no translation)
        target_lang = await detect_language(target_text)
        need_translation = False
    
    try:
        # 1. Translate if needed
        if need_translation:
            status_msg = await context.bot.send_message(
                chat_id=msg.chat_id,
                text=get_msg("voice_translating", user_id).format(lang=LANG_NAMES.get(target_lang, target_lang)),
                reply_to_message_id=reply_target_id
            )
            translated_text = await translate_text(target_text, target_lang)
            await status_msg.edit_text(get_msg("voice_generating", user_id))
            target_text = translated_text
            voice_reply_to = reply_target_id
        else:
            voice_reply_to = reply_target_id
            
        # 2. Convert to speech
        audio_buffer = await text_to_speech(target_text, target_lang)
        
        # 3. Build caption with smart_split
        lang_name = LANG_NAMES.get(target_lang, target_lang)
        if need_translation:
            header = f"üéôÔ∏è <b>ÿØŸàÿ®ŸÑŸá ({lang_name}):</b>"
            overflow_title = "ÿßÿØÿßŸÖŸá ÿØŸàÿ®ŸÑŸá"
        else:
            header = f"üîä <b>ŸÜÿ≥ÿÆŸá ÿµŸàÿ™€å ({lang_name}):</b>"
            overflow_title = "ÿßÿØÿßŸÖŸá ŸÖÿ™ŸÜ"
            
        caption, overflow_text = smart_split(target_text, header=header, max_len=1024)
        
        # 4. Send Voice
        voice_msg = await context.bot.send_voice(
            chat_id=msg.chat_id,
            voice=audio_buffer,
            caption=caption,
            parse_mode='HTML',
            reply_to_message_id=voice_reply_to,
            read_timeout=90
        )
        
        # 5. Send overflow parts
        if overflow_text:
            remaining = overflow_text
            while remaining:
                chunk = remaining[:4000]
                remaining = remaining[4000:]
                await context.bot.send_message(
                    chat_id=msg.chat_id,
                    text=f"üìù <b>{overflow_title}:</b>\n\n{html.escape(chunk)}",
                    parse_mode='HTML',
                    reply_to_message_id=voice_msg.message_id
                )
        
        if 'status_msg' in locals():
            if not IS_DEV: await safe_delete(status_msg)
            
    except Exception as e:
        logger.error(f"Voice Error: {e}")
        await report_error_to_admin(context, user_id, "/voice", str(e))
        error_msg = get_msg("err_ai", user_id) if 'user_id' in locals() else "ÿÆÿ∑ÿß€å€å ÿ±ÿÆ ÿØÿßÿØ."
        if 'status_msg' in locals():
            if not IS_DEV: await safe_delete(status_msg)
        
        await reply_and_delete(update, context, error_msg, delay=10)


def main():
    if not TELEGRAM_TOKEN:
        print("‚ùå Error: TELEGRAM_BOT_TOKEN not found in .env")
        return

    print("üöÄ Starting SmartBot Core...")
    from telegram.ext import JobQueue
    app = (
        ApplicationBuilder()
        .token(TELEGRAM_TOKEN)
        .concurrent_updates(True)
        .job_queue(JobQueue())  # Enable JobQueue for countdown timers
        .build()
    )

    # Commands
    app.add_handler(CommandHandler("dl", cmd_download_handler))
    app.add_handler(CommandHandler("download", cmd_download_handler))
    app.add_handler(CommandHandler("start", cmd_start_handler))
    app.add_handler(CommandHandler("help", cmd_start_handler)) # Reuse start for help
    app.add_handler(CommandHandler("close", cmd_close_handler))
    app.add_handler(CommandHandler("status", cmd_status_handler))
    app.add_handler(CommandHandler("toggle_dl", cmd_toggle_dl_handler))
    app.add_handler(CommandHandler("toggle_fc", cmd_toggle_fc_handler))
    app.add_handler(CommandHandler("price", cmd_price_handler))
    app.add_handler(CommandHandler("p", cmd_price_handler))
    app.add_handler(CommandHandler("check", cmd_check_handler))
    app.add_handler(CommandHandler("detail", cmd_detail_handler))
    app.add_handler(CommandHandler("voice", cmd_voice_handler))  # TTS Voice
    app.add_handler(CommandHandler("learn", cmd_learn_handler))
    app.add_handler(CommandHandler("l", cmd_learn_handler))
    app.add_handler(CommandHandler("t", cmd_learn_handler))  # /t is for /learn
    app.add_handler(CommandHandler("translate", cmd_learn_handler))
    app.add_handler(CommandHandler("edu", cmd_learn_handler))
    app.add_handler(CommandHandler("education", cmd_learn_handler))
    app.add_handler(CommandHandler("stop", cmd_stop_bot_handler))
    
    # All Messages (Text)
    app.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), global_message_handler))

    print("‚úÖ Bot is Polling...")
    app.run_polling(
        allowed_updates=["message", "callback_query"],  # Only listen to needed updates
        drop_pending_updates=True,  # Ignore old messages on restart
        close_loop=False  # Allow graceful shutdown
    )

if __name__ == "__main__":
    main()
